{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import List, Dict, Any, Union, Optional, Set, Tuple\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "config = {\n",
    "    'api_url': 'http://localhost:8081',\n",
    "    'logprobs': 100\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class TokenProb:\n",
    "    token: str\n",
    "    log_prob: float\n",
    "\n",
    "\n",
    "def get_logprobs_completion(prompt: Union[str, List[int]], config: Dict[str, Any]) -> List[TokenProb]:\n",
    "    url: str = config['api_url'] + '/completion'\n",
    "    logprobs: int = config['logprobs']    \n",
    "    \n",
    "    payload: Dict[str, Union[str, bool, float, int]] = {\n",
    "        'prompt': prompt,\n",
    "        'cache_prompt': True,\n",
    "        'n_predict': 1,\n",
    "        'temperature': 1.0,\n",
    "        'samplers': ['temperature'],\n",
    "        'n_probs': logprobs\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response: requests.Response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        probs: List[Dict[str, Union[str, float]]] = response.json()['completion_probabilities'][0]['probs']\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching logprobs: {e}\")\n",
    "        probs = []\n",
    "    \n",
    "    return [TokenProb(prob['tok_str'], prob['prob']) for prob in probs]\n",
    "\n",
    "\n",
    "def XTC_sampler(token_probs: List[TokenProb], threshold: int = 0.2, n_tokens_above_threshold: int = 3) -> List[str]:\n",
    "    \"\"\"Exclude Top Choices sampler\"\"\"\n",
    "    n_above_threshold = sum(1 for token_prob in token_probs if token_prob.log_prob > threshold)\n",
    "    if n_above_threshold >= n_tokens_above_threshold:\n",
    "        token_probs = token_probs[n_above_threshold-1:]\n",
    "    return token_probs\n",
    "\n",
    "def compute_entropy_varentropy(token_probs: List[TokenProb]) -> float:\n",
    "    probs = [math.exp(token_prob.log_prob) for token_prob in token_probs]\n",
    "    log_probs = [token_prob.log_prob for token_prob in token_probs]\n",
    "\n",
    "    # entropy (E[log P(x)])\n",
    "    entropy = -sum(prob * log_prob for prob, log_prob in zip(probs, log_probs))\n",
    "\n",
    "    # expected square of log probability E[(log P(x))^2]\n",
    "    expected_log_prob_squared = sum(prob * (log_prob ** 2) for prob, log_prob in zip(probs, log_probs))\n",
    "\n",
    "    # varentropy = E[(log P(x))^2] - (E[log P(x)])^2\n",
    "    varentropy = expected_log_prob_squared - (entropy ** 2)\n",
    "\n",
    "    return varentropy, entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<\n"
     ]
    }
   ],
   "source": [
    "def get_logprobs_chat_completion(prompt: Union[str, List[int]], config: Dict[str, Any]) -> List[TokenProb]:\n",
    "    url: str = config['api_url'] + '/v1/chat/completions'\n",
    "    logprobs: int = config['logprobs']    \n",
    "    \n",
    "    payload: Dict[str, Union[str, bool, float, int]] = {\n",
    "        'prompt': prompt,\n",
    "        'cache_prompt': True,\n",
    "        'n_predict': 1,\n",
    "        'temperature': 1.0,\n",
    "        #'samplers': ['top_k'],\n",
    "        'n_probs': logprobs\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response: requests.Response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        probs: List[Dict[str, Union[str, float]]] = response.json()['completion_probabilities'][0]['probs']\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching logprobs: {e}\")\n",
    "        probs = []\n",
    "    \n",
    "    return [TokenProb(prob['tok_str'], prob['prob']) for prob in probs]\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a princial engineer at Google, capable of complex reasoning and reflection. Reason through the query inside <thinking> tags process the question carefully and think step-by-step about the solution, and then provide your final response inside <output> tags. If you detect that you made a mistake in your reasoning at any point, correct yourself inside <reflection> tags.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Which number is larger, 9.9 or 9.11?\"},\n",
    "]\n",
    "\n",
    "payload: Dict[str, Union[str, bool, float, int]] = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": messages,\n",
    "        'cache_prompt': True,\n",
    "        'n_probs': 10,\n",
    "        'n_predict': 1,\n",
    "}\n",
    "\n",
    "url: str = config['api_url'] + '/v1/chat/completions'\n",
    "response: requests.Response = requests.post(url, json=payload)\n",
    "response.raise_for_status()\n",
    "\n",
    "print(response.json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'choices': [{'finish_reason': 'stop',\n",
       "   'index': 0,\n",
       "   'message': {'content': \"To compare 9.9 and 9.11, we need to compare their decimal expansions.\\n\\n9.9 can be written as 9 + 0.1 (where the + sign indicates adding)\\n\\n9.11 can be written as 9 + 0.1 + 0.1 (where the + sign indicates adding)\\n\\nBoth numbers are equal to 9. Since 9 is less than 9.1, 9.9 is also less than 9.11, and therefore 9.9 is larger than 9.11.\\n\\n<reflection> I initially considered comparing the numbers as decimals, but that led to a different conclusion. In reality, we should have compared their integer parts (9 and 9) and then compared their fractional parts (0.1 and 0.1), but that didn't change the outcome. It was a mistake to initially consider comparing the numbers as decimals.\",\n",
       "    'role': 'assistant'}}],\n",
       " 'created': 1727953433,\n",
       " 'model': 'gpt-3.5-turbo',\n",
       " 'object': 'chat.completion',\n",
       " 'usage': {'completion_tokens': 189,\n",
       "  'prompt_tokens': 103,\n",
       "  'total_tokens': 292},\n",
       " 'id': 'chatcmpl-kOopC0AO5pnxPxDNPeVPSuO396EppItB',\n",
       " 'completion_probabilities': [{'content': 'To',\n",
       "   'probs': [{'tok_str': '<', 'prob': 0.5479884147644043},\n",
       "    {'tok_str': 'To', 'prob': 0.2758261561393738},\n",
       "    {'tok_str': 'A', 'prob': 0.06916841864585876},\n",
       "    {'tok_str': '**', 'prob': 0.04218345135450363},\n",
       "    {'tok_str': 'Reflection', 'prob': 0.036465954035520554},\n",
       "    {'tok_str': 'Let', 'prob': 0.02836761064827442},\n",
       "    {'tok_str': 'I', 'prob': 0.0},\n",
       "    {'tok_str': 'The', 'prob': 0.0},\n",
       "    {'tok_str': '_<', 'prob': 0.0},\n",
       "    {'tok_str': 'Step', 'prob': 0.0}]},\n",
       "  {'content': ' compare',\n",
       "   'probs': [{'tok_str': ' determine', 'prob': 0.6125360131263733},\n",
       "    {'tok_str': ' compare', 'prob': 0.3874639868736267},\n",
       "    {'tok_str': ' answer', 'prob': 0.0},\n",
       "    {'tok_str': ' solve', 'prob': 0.0},\n",
       "    {'tok_str': ' decide', 'prob': 0.0},\n",
       "    {'tok_str': ' find', 'prob': 0.0},\n",
       "    {'tok_str': ' make', 'prob': 0.0},\n",
       "    {'tok_str': ' accurately', 'prob': 0.0},\n",
       "    {'tok_str': ' process', 'prob': 0.0},\n",
       "    {'tok_str': ' evaluate', 'prob': 0.0}]},\n",
       "  {'content': ' ',\n",
       "   'probs': [{'tok_str': ' these', 'prob': 0.4768023192882538},\n",
       "    {'tok_str': ' ', 'prob': 0.35413479804992676},\n",
       "    {'tok_str': ' the', 'prob': 0.16906292736530304},\n",
       "    {'tok_str': ' two', 'prob': 0.0},\n",
       "    {'tok_str': ' numbers', 'prob': 0.0},\n",
       "    {'tok_str': ' this', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': ' `', 'prob': 0.0},\n",
       "    {'tok_str': ' $', 'prob': 0.0},\n",
       "    {'tok_str': ' a', 'prob': 0.0}]},\n",
       "  {'content': '9',\n",
       "   'probs': [{'tok_str': '9', 'prob': 1.0},\n",
       "    {'tok_str': '0', 'prob': 0.0},\n",
       "    {'tok_str': '99', 'prob': 0.0},\n",
       "    {'tok_str': '09', 'prob': 0.0},\n",
       "    {'tok_str': '8', 'prob': 0.0},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': '4', 'prob': 0.0},\n",
       "    {'tok_str': '2', 'prob': 0.0},\n",
       "    {'tok_str': '89', 'prob': 0.0},\n",
       "    {'tok_str': '5', 'prob': 0.0}]},\n",
       "  {'content': '.',\n",
       "   'probs': [{'tok_str': '.', 'prob': 1.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': ' .', 'prob': 0.0},\n",
       "    {'tok_str': '.\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': '-', 'prob': 0.0},\n",
       "    {'tok_str': '.x', 'prob': 0.0},\n",
       "    {'tok_str': ':', 'prob': 0.0},\n",
       "    {'tok_str': '/', 'prob': 0.0},\n",
       "    {'tok_str': ' point', 'prob': 0.0}]},\n",
       "  {'content': '9',\n",
       "   'probs': [{'tok_str': '9', 'prob': 1.0},\n",
       "    {'tok_str': '99', 'prob': 0.0},\n",
       "    {'tok_str': '90', 'prob': 0.0},\n",
       "    {'tok_str': '91', 'prob': 0.0},\n",
       "    {'tok_str': '8', 'prob': 0.0},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': '93', 'prob': 0.0},\n",
       "    {'tok_str': '4', 'prob': 0.0},\n",
       "    {'tok_str': '95', 'prob': 0.0},\n",
       "    {'tok_str': '92', 'prob': 0.0}]},\n",
       "  {'content': ' and',\n",
       "   'probs': [{'tok_str': ' and', 'prob': 1.0},\n",
       "    {'tok_str': ' with', 'prob': 0.0},\n",
       "    {'tok_str': ' to', 'prob': 0.0},\n",
       "    {'tok_str': ' versus', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' or', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': ' vs', 'prob': 0.0},\n",
       "    {'tok_str': ' against', 'prob': 0.0},\n",
       "    {'tok_str': ' from', 'prob': 0.0}]},\n",
       "  {'content': ' ',\n",
       "   'probs': [{'tok_str': ' ', 'prob': 1.0},\n",
       "    {'tok_str': ' the', 'prob': 0.0},\n",
       "    {'tok_str': ' its', 'prob': 0.0},\n",
       "    {'tok_str': ' a', 'prob': 0.0},\n",
       "    {'tok_str': ' approximately', 'prob': 0.0},\n",
       "    {'tok_str': ' -', 'prob': 0.0},\n",
       "    {'tok_str': ' **', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' $', 'prob': 0.0},\n",
       "    {'tok_str': ' another', 'prob': 0.0}]},\n",
       "  {'content': '9',\n",
       "   'probs': [{'tok_str': '9', 'prob': 1.0},\n",
       "    {'tok_str': '0', 'prob': 0.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': ' ', 'prob': 0.0},\n",
       "    {'tok_str': '8', 'prob': 0.0},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': '4', 'prob': 0.0},\n",
       "    {'tok_str': '09', 'prob': 0.0},\n",
       "    {'tok_str': '99', 'prob': 0.0},\n",
       "    {'tok_str': '2', 'prob': 0.0}]},\n",
       "  {'content': '.',\n",
       "   'probs': [{'tok_str': '.', 'prob': 1.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': '.\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': '-', 'prob': 0.0},\n",
       "    {'tok_str': '·', 'prob': 0.0},\n",
       "    {'tok_str': ':', 'prob': 0.0},\n",
       "    {'tok_str': ' ', 'prob': 0.0},\n",
       "    {'tok_str': ' .', 'prob': 0.0},\n",
       "    {'tok_str': '/', 'prob': 0.0},\n",
       "    {'tok_str': ' with', 'prob': 0.0}]},\n",
       "  {'content': '11',\n",
       "   'probs': [{'tok_str': '11', 'prob': 1.0},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': '111', 'prob': 0.0},\n",
       "    {'tok_str': '121', 'prob': 0.0},\n",
       "    {'tok_str': '12', 'prob': 0.0},\n",
       "    {'tok_str': '101', 'prob': 0.0},\n",
       "    {'tok_str': '112', 'prob': 0.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': ' ', 'prob': 0.0},\n",
       "    {'tok_str': '9', 'prob': 0.0}]},\n",
       "  {'content': ',',\n",
       "   'probs': [{'tok_str': ',', 'prob': 1.0},\n",
       "    {'tok_str': ' accurately', 'prob': 0.0},\n",
       "    {'tok_str': ' more', 'prob': 0.0},\n",
       "    {'tok_str': ' numer', 'prob': 0.0},\n",
       "    {'tok_str': ' without', 'prob': 0.0},\n",
       "    {'tok_str': ' correctly', 'prob': 0.0},\n",
       "    {'tok_str': ' directly', 'prob': 0.0},\n",
       "    {'tok_str': ' easily', 'prob': 0.0},\n",
       "    {'tok_str': ' precisely', 'prob': 0.0},\n",
       "    {'tok_str': ' we', 'prob': 0.0}]},\n",
       "  {'content': ' we',\n",
       "   'probs': [{'tok_str': ' we', 'prob': 0.5832381248474121},\n",
       "    {'tok_str': ' let', 'prob': 0.3083289861679077},\n",
       "    {'tok_str': ' I', 'prob': 0.108432836830616},\n",
       "    {'tok_str': ' it', 'prob': 0.0},\n",
       "    {'tok_str': ' first', 'prob': 0.0},\n",
       "    {'tok_str': ' notice', 'prob': 0.0},\n",
       "    {'tok_str': ' you', 'prob': 0.0},\n",
       "    {'tok_str': ' consider', 'prob': 0.0},\n",
       "    {'tok_str': ' subtract', 'prob': 0.0},\n",
       "    {'tok_str': ' the', 'prob': 0.0}]},\n",
       "  {'content': ' need',\n",
       "   'probs': [{'tok_str': ' need', 'prob': 0.6271255612373352},\n",
       "    {'tok_str': ' can', 'prob': 0.3728744089603424},\n",
       "    {'tok_str': ' should', 'prob': 0.0},\n",
       "    {'tok_str': ' only', 'prob': 0.0},\n",
       "    {'tok_str': ' first', 'prob': 0.0},\n",
       "    {'tok_str': ' simply', 'prob': 0.0},\n",
       "    {'tok_str': ' must', 'prob': 0.0},\n",
       "    {'tok_str': \"'ll\", 'prob': 0.0},\n",
       "    {'tok_str': ' just', 'prob': 0.0},\n",
       "    {'tok_str': ' don', 'prob': 0.0}]},\n",
       "  {'content': ' to',\n",
       "   'probs': [{'tok_str': ' to', 'prob': 1.0},\n",
       "    {'tok_str': ' only', 'prob': 0.0},\n",
       "    {'tok_str': ' a', 'prob': 0.0},\n",
       "    {'tok_str': ' not', 'prob': 0.0},\n",
       "    {'tok_str': ' two', 'prob': 0.0},\n",
       "    {'tok_str': ' the', 'prob': 0.0},\n",
       "    {'tok_str': ' more', 'prob': 0.0},\n",
       "    {'tok_str': ' simply', 'prob': 0.0},\n",
       "    {'tok_str': ' at', 'prob': 0.0},\n",
       "    {'tok_str': ' look', 'prob': 0.0}]},\n",
       "  {'content': ' compare',\n",
       "   'probs': [{'tok_str': ' consider', 'prob': 0.2829504609107971},\n",
       "    {'tok_str': ' look', 'prob': 0.28196725249290466},\n",
       "    {'tok_str': ' compare', 'prob': 0.11302965879440308},\n",
       "    {'tok_str': ' find', 'prob': 0.10423271358013153},\n",
       "    {'tok_str': ' determine', 'prob': 0.07444573938846588},\n",
       "    {'tok_str': ' examine', 'prob': 0.03393454849720001},\n",
       "    {'tok_str': ' make', 'prob': 0.021737506613135338},\n",
       "    {'tok_str': ' round', 'prob': 0.01774691417813301},\n",
       "    {'tok_str': ' convert', 'prob': 0.0159720778465271},\n",
       "    {'tok_str': ' analyze', 'prob': 0.014507723972201347}]},\n",
       "  {'content': ' their',\n",
       "   'probs': [{'tok_str': ' their', 'prob': 0.6493832468986511},\n",
       "    {'tok_str': ' the', 'prob': 0.35061681270599365},\n",
       "    {'tok_str': ' them', 'prob': 0.0},\n",
       "    {'tok_str': ' ', 'prob': 0.0},\n",
       "    {'tok_str': ' two', 'prob': 0.0},\n",
       "    {'tok_str': ' decimal', 'prob': 0.0},\n",
       "    {'tok_str': ' floating', 'prob': 0.0},\n",
       "    {'tok_str': ' these', 'prob': 0.0},\n",
       "    {'tok_str': ' numbers', 'prob': 0.0},\n",
       "    {'tok_str': ' both', 'prob': 0.0}]},\n",
       "  {'content': ' decimal',\n",
       "   'probs': [{'tok_str': ' decimal', 'prob': 1.0},\n",
       "    {'tok_str': ' numerical', 'prob': 0.0},\n",
       "    {'tok_str': ' values', 'prob': 0.0},\n",
       "    {'tok_str': ' absolute', 'prob': 0.0},\n",
       "    {'tok_str': ' decimals', 'prob': 0.0},\n",
       "    {'tok_str': ' differences', 'prob': 0.0},\n",
       "    {'tok_str': ' actual', 'prob': 0.0},\n",
       "    {'tok_str': ' fractional', 'prob': 0.0},\n",
       "    {'tok_str': ' digits', 'prob': 0.0},\n",
       "    {'tok_str': ' numbers', 'prob': 0.0}]},\n",
       "  {'content': ' expansions',\n",
       "   'probs': [{'tok_str': ' parts', 'prob': 0.5974659323692322},\n",
       "    {'tok_str': ' representations', 'prob': 0.16753654181957245},\n",
       "    {'tok_str': ' values', 'prob': 0.14447566866874695},\n",
       "    {'tok_str': ' expansions', 'prob': 0.07335816323757172},\n",
       "    {'tok_str': ' equivalents', 'prob': 0.017163708806037903},\n",
       "    {'tok_str': ' places', 'prob': 0.0},\n",
       "    {'tok_str': ' digits', 'prob': 0.0},\n",
       "    {'tok_str': ' approx', 'prob': 0.0},\n",
       "    {'tok_str': ' points', 'prob': 0.0},\n",
       "    {'tok_str': ' representation', 'prob': 0.0}]},\n",
       "  {'content': '.\\n\\n',\n",
       "   'probs': [{'tok_str': '.\\n\\n', 'prob': 0.7932161092758179},\n",
       "    {'tok_str': '.', 'prob': 0.20678392052650452},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': ':\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': ' to', 'prob': 0.0},\n",
       "    {'tok_str': ' or', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': ' as', 'prob': 0.0},\n",
       "    {'tok_str': ' directly', 'prob': 0.0},\n",
       "    {'tok_str': ' with', 'prob': 0.0}]},\n",
       "  {'content': '9',\n",
       "   'probs': [{'tok_str': '9', 'prob': 1.0},\n",
       "    {'tok_str': 'First', 'prob': 0.0},\n",
       "    {'tok_str': 'Since', 'prob': 0.0},\n",
       "    {'tok_str': 'The', 'prob': 0.0},\n",
       "    {'tok_str': 'We', 'prob': 0.0},\n",
       "    {'tok_str': 'Let', 'prob': 0.0},\n",
       "    {'tok_str': 'In', 'prob': 0.0},\n",
       "    {'tok_str': 'Both', 'prob': 0.0},\n",
       "    {'tok_str': 'Here', 'prob': 0.0},\n",
       "    {'tok_str': '1', 'prob': 0.0}]},\n",
       "  {'content': '.',\n",
       "   'probs': [{'tok_str': '.', 'prob': 1.0},\n",
       "    {'tok_str': ' is', 'prob': 0.0},\n",
       "    {'tok_str': ' can', 'prob': 0.0},\n",
       "    {'tok_str': ' =', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': ' decimal', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' +', 'prob': 0.0},\n",
       "    {'tok_str': ' has', 'prob': 0.0}]},\n",
       "  {'content': '9',\n",
       "   'probs': [{'tok_str': '9', 'prob': 1.0},\n",
       "    {'tok_str': '99', 'prob': 0.0},\n",
       "    {'tok_str': '11', 'prob': 0.0},\n",
       "    {'tok_str': '0', 'prob': 0.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': '90', 'prob': 0.0},\n",
       "    {'tok_str': '100', 'prob': 0.0},\n",
       "    {'tok_str': '999', 'prob': 0.0},\n",
       "    {'tok_str': '8', 'prob': 0.0}]},\n",
       "  {'content': ' can',\n",
       "   'probs': [{'tok_str': ' can', 'prob': 0.3315027356147766},\n",
       "    {'tok_str': ' =', 'prob': 0.2828606069087982},\n",
       "    {'tok_str': ' is', 'prob': 0.22388964891433716},\n",
       "    {'tok_str': ' has', 'prob': 0.10199766606092453},\n",
       "    {'tok_str': ' and', 'prob': 0.04863092675805092},\n",
       "    {'tok_str': ':', 'prob': 0.01111837849020958},\n",
       "    {'tok_str': ' ≈', 'prob': 0.0},\n",
       "    {'tok_str': ' <', 'prob': 0.0},\n",
       "    {'tok_str': ' in', 'prob': 0.0},\n",
       "    {'tok_str': '\\n', 'prob': 0.0}]},\n",
       "  {'content': ' be',\n",
       "   'probs': [{'tok_str': ' be', 'prob': 1.0},\n",
       "    {'tok_str': ' also', 'prob': 0.0},\n",
       "    {'tok_str': ' still', 'prob': 0.0},\n",
       "    {'tok_str': ' been', 'prob': 0.0},\n",
       "    {'tok_str': ' have', 'prob': 0.0},\n",
       "    {'tok_str': ' expanded', 'prob': 0.0},\n",
       "    {'tok_str': ' represent', 'prob': 0.0},\n",
       "    {'tok_str': ' only', 'prob': 0.0},\n",
       "    {'tok_str': ' written', 'prob': 0.0},\n",
       "    {'tok_str': \"'t\", 'prob': 0.0}]},\n",
       "  {'content': ' written',\n",
       "   'probs': [{'tok_str': ' written', 'prob': 0.7930577993392944},\n",
       "    {'tok_str': ' represented', 'prob': 0.11844184994697571},\n",
       "    {'tok_str': ' expressed', 'prob': 0.06549756973981857},\n",
       "    {'tok_str': ' expanded', 'prob': 0.023002730682492256},\n",
       "    {'tok_str': ' broken', 'prob': 0.0},\n",
       "    {'tok_str': ' rewritten', 'prob': 0.0},\n",
       "    {'tok_str': ' approx', 'prob': 0.0},\n",
       "    {'tok_str': ' thought', 'prob': 0.0},\n",
       "    {'tok_str': ' seen', 'prob': 0.0},\n",
       "    {'tok_str': ' read', 'prob': 0.0}]},\n",
       "  {'content': ' as',\n",
       "   'probs': [{'tok_str': ' as', 'prob': 1.0},\n",
       "    {'tok_str': ' in', 'prob': 0.0},\n",
       "    {'tok_str': ' exactly', 'prob': 0.0},\n",
       "    {'tok_str': ' with', 'prob': 0.0},\n",
       "    {'tok_str': ' numer', 'prob': 0.0},\n",
       "    {'tok_str': ' to', 'prob': 0.0},\n",
       "    {'tok_str': ' ', 'prob': 0.0},\n",
       "    {'tok_str': ' using', 'prob': 0.0},\n",
       "    {'tok_str': ':', 'prob': 0.0},\n",
       "    {'tok_str': ' directly', 'prob': 0.0}]},\n",
       "  {'content': ' ',\n",
       "   'probs': [{'tok_str': ' ', 'prob': 1.0},\n",
       "    {'tok_str': ':', 'prob': 0.0},\n",
       "    {'tok_str': ' the', 'prob': 0.0},\n",
       "    {'tok_str': ' .', 'prob': 0.0},\n",
       "    {'tok_str': ' $', 'prob': 0.0},\n",
       "    {'tok_str': ' a', 'prob': 0.0},\n",
       "    {'tok_str': ':\\n', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' `', 'prob': 0.0},\n",
       "    {'tok_str': ':\\n\\n', 'prob': 0.0}]},\n",
       "  {'content': '9',\n",
       "   'probs': [{'tok_str': '9', 'prob': 0.9340330958366394},\n",
       "    {'tok_str': '99', 'prob': 0.03517619147896767},\n",
       "    {'tok_str': '999', 'prob': 0.030790727585554123},\n",
       "    {'tok_str': '990', 'prob': 0.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': '109', 'prob': 0.0},\n",
       "    {'tok_str': '0', 'prob': 0.0},\n",
       "    {'tok_str': '100', 'prob': 0.0},\n",
       "    {'tok_str': '991', 'prob': 0.0}]},\n",
       "  {'content': ' +',\n",
       "   'probs': [{'tok_str': ' +', 'prob': 0.9312372803688049},\n",
       "    {'tok_str': '.', 'prob': 0.06876273453235626},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': '+', 'prob': 0.0},\n",
       "    {'tok_str': ' with', 'prob': 0.0},\n",
       "    {'tok_str': ' ×', 'prob': 0.0},\n",
       "    {'tok_str': ' ', 'prob': 0.0},\n",
       "    {'tok_str': '(', 'prob': 0.0},\n",
       "    {'tok_str': ' x', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0}]},\n",
       "  {'content': ' ',\n",
       "   'probs': [{'tok_str': ' ', 'prob': 1.0},\n",
       "    {'tok_str': ' .', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': '0', 'prob': 0.0},\n",
       "    {'tok_str': ' x', 'prob': 0.0},\n",
       "    {'tok_str': ' decimal', 'prob': 0.0},\n",
       "    {'tok_str': ' √', 'prob': 0.0},\n",
       "    {'tok_str': ' $', 'prob': 0.0},\n",
       "    {'tok_str': ' tent', 'prob': 0.0},\n",
       "    {'tok_str': ' �', 'prob': 0.0}]},\n",
       "  {'content': '0',\n",
       "   'probs': [{'tok_str': '0', 'prob': 1.0},\n",
       "    {'tok_str': '9', 'prob': 0.0},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': '90', 'prob': 0.0},\n",
       "    {'tok_str': '99', 'prob': 0.0},\n",
       "    {'tok_str': '100', 'prob': 0.0},\n",
       "    {'tok_str': '00', 'prob': 0.0},\n",
       "    {'tok_str': '2', 'prob': 0.0},\n",
       "    {'tok_str': '4', 'prob': 0.0}]},\n",
       "  {'content': '.',\n",
       "   'probs': [{'tok_str': '.', 'prob': 1.0},\n",
       "    {'tok_str': ' point', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': '.x', 'prob': 0.0},\n",
       "    {'tok_str': ' .', 'prob': 0.0},\n",
       "    {'tok_str': ' =', 'prob': 0.0},\n",
       "    {'tok_str': ' +', 'prob': 0.0},\n",
       "    {'tok_str': '.\\n', 'prob': 0.0},\n",
       "    {'tok_str': '/', 'prob': 0.0},\n",
       "    {'tok_str': '·', 'prob': 0.0}]},\n",
       "  {'content': '1',\n",
       "   'probs': [{'tok_str': '9', 'prob': 0.5153229236602783},\n",
       "    {'tok_str': '1', 'prob': 0.3884122967720032},\n",
       "    {'tok_str': '99', 'prob': 0.09626484662294388},\n",
       "    {'tok_str': '999', 'prob': 0.0},\n",
       "    {'tok_str': '11', 'prob': 0.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': '90', 'prob': 0.0},\n",
       "    {'tok_str': '5', 'prob': 0.0},\n",
       "    {'tok_str': '01', 'prob': 0.0},\n",
       "    {'tok_str': '09', 'prob': 0.0}]},\n",
       "  {'content': ' (',\n",
       "   'probs': [{'tok_str': ',', 'prob': 0.5410066246986389},\n",
       "    {'tok_str': '\\n', 'prob': 0.19499562680721283},\n",
       "    {'tok_str': ' (', 'prob': 0.08521263301372528},\n",
       "    {'tok_str': '\\n\\n', 'prob': 0.07395512610673904},\n",
       "    {'tok_str': '.\\n\\n', 'prob': 0.0359317883849144},\n",
       "    {'tok_str': '.\\n', 'prob': 0.026813138276338577},\n",
       "    {'tok_str': '.', 'prob': 0.025882869958877563},\n",
       "    {'tok_str': ' and', 'prob': 0.016202183440327644},\n",
       "    {'tok_str': ' =', 'prob': 0.0},\n",
       "    {'tok_str': ' or', 'prob': 0.0}]},\n",
       "  {'content': 'where',\n",
       "   'probs': [{'tok_str': 'where', 'prob': 0.3643636107444763},\n",
       "    {'tok_str': 'since', 'prob': 0.29235976934432983},\n",
       "    {'tok_str': 'or', 'prob': 0.0721401795744896},\n",
       "    {'tok_str': 'con', 'prob': 0.050168104469776154},\n",
       "    {'tok_str': '9', 'prob': 0.038516171276569366},\n",
       "    {'tok_str': 'in', 'prob': 0.03818859905004501},\n",
       "    {'tok_str': 'the', 'prob': 0.03731883689761162},\n",
       "    {'tok_str': 'a', 'prob': 0.023026006296277046},\n",
       "    {'tok_str': '1', 'prob': 0.018617140129208565},\n",
       "    {'tok_str': 'one', 'prob': 0.015855377539992332}]},\n",
       "  {'content': ' the',\n",
       "   'probs': [{'tok_str': ' ', 'prob': 0.8728141188621521},\n",
       "    {'tok_str': ' the', 'prob': 0.1271858662366867},\n",
       "    {'tok_str': ' +', 'prob': 0.0},\n",
       "    {'tok_str': ' we', 'prob': 0.0},\n",
       "    {'tok_str': ' \"', 'prob': 0.0},\n",
       "    {'tok_str': ' each', 'prob': 0.0},\n",
       "    {'tok_str': ' .', 'prob': 0.0},\n",
       "    {'tok_str': ' both', 'prob': 0.0},\n",
       "    {'tok_str': ' a', 'prob': 0.0},\n",
       "    {'tok_str': ' \"+\"', 'prob': 0.0}]},\n",
       "  {'content': ' +',\n",
       "   'probs': [{'tok_str': ' ', 'prob': 0.5643665194511414},\n",
       "    {'tok_str': ' +', 'prob': 0.17843428254127502},\n",
       "    {'tok_str': ' decimal', 'prob': 0.15695255994796753},\n",
       "    {'tok_str': ' \"', 'prob': 0.05934203043580055},\n",
       "    {'tok_str': ' dots', 'prob': 0.02508796751499176},\n",
       "    {'tok_str': ' dot', 'prob': 0.015816686674952507},\n",
       "    {'tok_str': ' digits', 'prob': 0.0},\n",
       "    {'tok_str': ' .', 'prob': 0.0},\n",
       "    {'tok_str': ' first', 'prob': 0.0},\n",
       "    {'tok_str': ' tent', 'prob': 0.0}]},\n",
       "  {'content': ' sign',\n",
       "   'probs': [{'tok_str': ' sign', 'prob': 0.7111819386482239},\n",
       "    {'tok_str': ' symbol', 'prob': 0.14547285437583923},\n",
       "    {'tok_str': ' is', 'prob': 0.08359795808792114},\n",
       "    {'tok_str': ' represents', 'prob': 0.038764189928770065},\n",
       "    {'tok_str': ' and', 'prob': 0.020983070135116577},\n",
       "    {'tok_str': ' denotes', 'prob': 0.0},\n",
       "    {'tok_str': ' indicates', 'prob': 0.0},\n",
       "    {'tok_str': ' signs', 'prob': 0.0},\n",
       "    {'tok_str': '0', 'prob': 0.0},\n",
       "    {'tok_str': ' operator', 'prob': 0.0}]},\n",
       "  {'content': ' indicates',\n",
       "   'probs': [{'tok_str': ' indicates', 'prob': 0.6890852451324463},\n",
       "    {'tok_str': ' represents', 'prob': 0.1745966225862503},\n",
       "    {'tok_str': ' denotes', 'prob': 0.048911549150943756},\n",
       "    {'tok_str': ' separates', 'prob': 0.04041081666946411},\n",
       "    {'tok_str': ' is', 'prob': 0.024665631353855133},\n",
       "    {'tok_str': ' means', 'prob': 0.022330131381750107},\n",
       "    {'tok_str': ' shows', 'prob': 0.0},\n",
       "    {'tok_str': ' implies', 'prob': 0.0},\n",
       "    {'tok_str': ' stands', 'prob': 0.0},\n",
       "    {'tok_str': ' adds', 'prob': 0.0}]},\n",
       "  {'content': ' adding',\n",
       "   'probs': [{'tok_str': ' addition', 'prob': 0.7356185913085938},\n",
       "    {'tok_str': ' \"', 'prob': 0.07399100065231323},\n",
       "    {'tok_str': ' a', 'prob': 0.06715340912342072},\n",
       "    {'tok_str': ' that', 'prob': 0.04587490111589432},\n",
       "    {'tok_str': ' adding', 'prob': 0.0296266321092844},\n",
       "    {'tok_str': ' an', 'prob': 0.02494560368359089},\n",
       "    {'tok_str': ' the', 'prob': 0.02278982289135456},\n",
       "    {'tok_str': ' decimal', 'prob': 0.0},\n",
       "    {'tok_str': ' concaten', 'prob': 0.0},\n",
       "    {'tok_str': ' rounding', 'prob': 0.0}]},\n",
       "  {'content': ')\\n\\n',\n",
       "   'probs': [{'tok_str': ' ', 'prob': 0.3436972498893738},\n",
       "    {'tok_str': ' the', 'prob': 0.3266322612762451},\n",
       "    {'tok_str': ' a', 'prob': 0.11437869071960449},\n",
       "    {'tok_str': ')\\n', 'prob': 0.05287795886397362},\n",
       "    {'tok_str': ' one', 'prob': 0.026824703440070152},\n",
       "    {'tok_str': ').\\n', 'prob': 0.024964870885014534},\n",
       "    {'tok_str': '),', 'prob': 0.0196596160531044},\n",
       "    {'tok_str': ')\\n\\n', 'prob': 0.017940549179911613},\n",
       "    {'tok_str': ' together', 'prob': 0.016508327797055244},\n",
       "    {'tok_str': ' or', 'prob': 0.014406923204660416}]},\n",
       "  {'content': '9',\n",
       "   'probs': [{'tok_str': '9', 'prob': 1.0},\n",
       "    {'tok_str': 'Similarly', 'prob': 0.0},\n",
       "    {'tok_str': 'On', 'prob': 0.0},\n",
       "    {'tok_str': 'And', 'prob': 0.0},\n",
       "    {'tok_str': 'Now', 'prob': 0.0},\n",
       "    {'tok_str': 'Since', 'prob': 0.0},\n",
       "    {'tok_str': 'So', 'prob': 0.0},\n",
       "    {'tok_str': 'The', 'prob': 0.0},\n",
       "    {'tok_str': '0', 'prob': 0.0},\n",
       "    {'tok_str': 'Con', 'prob': 0.0}]},\n",
       "  {'content': '.',\n",
       "   'probs': [{'tok_str': '.', 'prob': 1.0},\n",
       "    {'tok_str': ' +', 'prob': 0.0},\n",
       "    {'tok_str': ' is', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': ' =', 'prob': 0.0},\n",
       "    {'tok_str': ' can', 'prob': 0.0},\n",
       "    {'tok_str': '.\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': '+', 'prob': 0.0},\n",
       "    {'tok_str': '.x', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0}]},\n",
       "  {'content': '11',\n",
       "   'probs': [{'tok_str': '11', 'prob': 1.0},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': '0', 'prob': 0.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': '9', 'prob': 0.0},\n",
       "    {'tok_str': '99', 'prob': 0.0},\n",
       "    {'tok_str': '111', 'prob': 0.0},\n",
       "    {'tok_str': '101', 'prob': 0.0},\n",
       "    {'tok_str': '12', 'prob': 0.0},\n",
       "    {'tok_str': '121', 'prob': 0.0}]},\n",
       "  {'content': ' can',\n",
       "   'probs': [{'tok_str': ' can', 'prob': 1.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': ' is', 'prob': 0.0},\n",
       "    {'tok_str': ' cannot', 'prob': 0.0},\n",
       "    {'tok_str': ' =', 'prob': 0.0},\n",
       "    {'tok_str': ' has', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' remains', 'prob': 0.0},\n",
       "    {'tok_str': ' does', 'prob': 0.0},\n",
       "    {'tok_str': ' requires', 'prob': 0.0}]},\n",
       "  {'content': ' be',\n",
       "   'probs': [{'tok_str': ' be', 'prob': 1.0},\n",
       "    {'tok_str': ' also', 'prob': 0.0},\n",
       "    {'tok_str': ' then', 'prob': 0.0},\n",
       "    {'tok_str': ' already', 'prob': 0.0},\n",
       "    {'tok_str': ' only', 'prob': 0.0},\n",
       "    {'tok_str': ' similarly', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': ' now', 'prob': 0.0},\n",
       "    {'tok_str': ' always', 'prob': 0.0},\n",
       "    {'tok_str': ' clearly', 'prob': 0.0}]},\n",
       "  {'content': ' written',\n",
       "   'probs': [{'tok_str': ' written', 'prob': 1.0},\n",
       "    {'tok_str': ' expressed', 'prob': 0.0},\n",
       "    {'tok_str': ' rewritten', 'prob': 0.0},\n",
       "    {'tok_str': ' expanded', 'prob': 0.0},\n",
       "    {'tok_str': ' read', 'prob': 0.0},\n",
       "    {'tok_str': ' represented', 'prob': 0.0},\n",
       "    {'tok_str': ' broken', 'prob': 0.0},\n",
       "    {'tok_str': ' compared', 'prob': 0.0},\n",
       "    {'tok_str': ' seen', 'prob': 0.0},\n",
       "    {'tok_str': ' directly', 'prob': 0.0}]},\n",
       "  {'content': ' as',\n",
       "   'probs': [{'tok_str': ' as', 'prob': 1.0},\n",
       "    {'tok_str': ' directly', 'prob': 0.0},\n",
       "    {'tok_str': ' the', 'prob': 0.0},\n",
       "    {'tok_str': ' exactly', 'prob': 0.0},\n",
       "    {'tok_str': ' in', 'prob': 0.0},\n",
       "    {'tok_str': ' is', 'prob': 0.0},\n",
       "    {'tok_str': ' similarly', 'prob': 0.0},\n",
       "    {'tok_str': ' with', 'prob': 0.0},\n",
       "    {'tok_str': ' by', 'prob': 0.0},\n",
       "    {'tok_str': ' more', 'prob': 0.0}]},\n",
       "  {'content': ' ',\n",
       "   'probs': [{'tok_str': ' ', 'prob': 1.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' the', 'prob': 0.0},\n",
       "    {'tok_str': ' exactly', 'prob': 0.0},\n",
       "    {'tok_str': ' a', 'prob': 0.0},\n",
       "    {'tok_str': ':\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': ' simply', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': ':\\n', 'prob': 0.0},\n",
       "    {'tok_str': ':', 'prob': 0.0}]},\n",
       "  {'content': '9',\n",
       "   'probs': [{'tok_str': '9', 'prob': 1.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': '0', 'prob': 0.0},\n",
       "    {'tok_str': '8', 'prob': 0.0},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': '11', 'prob': 0.0},\n",
       "    {'tok_str': ' ', 'prob': 0.0},\n",
       "    {'tok_str': '91', 'prob': 0.0},\n",
       "    {'tok_str': '90', 'prob': 0.0},\n",
       "    {'tok_str': '99', 'prob': 0.0}]},\n",
       "  {'content': ' +',\n",
       "   'probs': [{'tok_str': ' +', 'prob': 1.0},\n",
       "    {'tok_str': '.', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': '+', 'prob': 0.0},\n",
       "    {'tok_str': ' plus', 'prob': 0.0},\n",
       "    {'tok_str': ' with', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': ' -', 'prob': 0.0},\n",
       "    {'tok_str': ' ', 'prob': 0.0}]},\n",
       "  {'content': ' ',\n",
       "   'probs': [{'tok_str': ' ', 'prob': 1.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' .', 'prob': 0.0},\n",
       "    {'tok_str': ' (-', 'prob': 0.0},\n",
       "    {'tok_str': ' x', 'prob': 0.0},\n",
       "    {'tok_str': ' +', 'prob': 0.0},\n",
       "    {'tok_str': ' (+', 'prob': 0.0},\n",
       "    {'tok_str': ' �', 'prob': 0.0},\n",
       "    {'tok_str': '0', 'prob': 0.0},\n",
       "    {'tok_str': ' �', 'prob': 0.0}]},\n",
       "  {'content': '0',\n",
       "   'probs': [{'tok_str': '0', 'prob': 1.0},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': '9', 'prob': 0.0},\n",
       "    {'tok_str': '11', 'prob': 0.0},\n",
       "    {'tok_str': '2', 'prob': 0.0},\n",
       "    {'tok_str': '5', 'prob': 0.0},\n",
       "    {'tok_str': '3', 'prob': 0.0},\n",
       "    {'tok_str': '8', 'prob': 0.0},\n",
       "    {'tok_str': '00', 'prob': 0.0}]},\n",
       "  {'content': '.',\n",
       "   'probs': [{'tok_str': '.', 'prob': 1.0},\n",
       "    {'tok_str': ' +', 'prob': 0.0},\n",
       "    {'tok_str': '.(', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': '+', 'prob': 0.0},\n",
       "    {'tok_str': ' .', 'prob': 0.0},\n",
       "    {'tok_str': '.x', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': '.\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': '.\\\\', 'prob': 0.0}]},\n",
       "  {'content': '1',\n",
       "   'probs': [{'tok_str': '1', 'prob': 0.9413233995437622},\n",
       "    {'tok_str': '11', 'prob': 0.05867660418152809},\n",
       "    {'tok_str': '2', 'prob': 0.0},\n",
       "    {'tok_str': '111', 'prob': 0.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': '12', 'prob': 0.0},\n",
       "    {'tok_str': '09', 'prob': 0.0},\n",
       "    {'tok_str': '100', 'prob': 0.0},\n",
       "    {'tok_str': '110', 'prob': 0.0},\n",
       "    {'tok_str': '01', 'prob': 0.0}]},\n",
       "  {'content': ' +',\n",
       "   'probs': [{'tok_str': ' +', 'prob': 0.9532968401908875},\n",
       "    {'tok_str': ' (', 'prob': 0.046703215688467026},\n",
       "    {'tok_str': '\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': ' =', 'prob': 0.0},\n",
       "    {'tok_str': '+', 'prob': 0.0},\n",
       "    {'tok_str': ' or', 'prob': 0.0},\n",
       "    {'tok_str': '.\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': ' as', 'prob': 0.0}]},\n",
       "  {'content': ' ',\n",
       "   'probs': [{'tok_str': ' ', 'prob': 1.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' .', 'prob': 0.0},\n",
       "    {'tok_str': ' ...', 'prob': 0.0},\n",
       "    {'tok_str': '0', 'prob': 0.0},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': ' additional', 'prob': 0.0},\n",
       "    {'tok_str': ' x', 'prob': 0.0},\n",
       "    {'tok_str': ' another', 'prob': 0.0},\n",
       "    {'tok_str': ' �', 'prob': 0.0}]},\n",
       "  {'content': '0',\n",
       "   'probs': [{'tok_str': '0', 'prob': 1.0},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': '11', 'prob': 0.0},\n",
       "    {'tok_str': '2', 'prob': 0.0},\n",
       "    {'tok_str': '5', 'prob': 0.0},\n",
       "    {'tok_str': '100', 'prob': 0.0},\n",
       "    {'tok_str': '01', 'prob': 0.0},\n",
       "    {'tok_str': '00', 'prob': 0.0},\n",
       "    {'tok_str': '110', 'prob': 0.0}]},\n",
       "  {'content': '.',\n",
       "   'probs': [{'tok_str': '.', 'prob': 1.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' +', 'prob': 0.0},\n",
       "    {'tok_str': '.(', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': '.\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': '\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': ' .', 'prob': 0.0},\n",
       "    {'tok_str': '+', 'prob': 0.0},\n",
       "    {'tok_str': ' point', 'prob': 0.0}]},\n",
       "  {'content': '1',\n",
       "   'probs': [{'tok_str': '1', 'prob': 0.8616374135017395},\n",
       "    {'tok_str': '01', 'prob': 0.10741709917783737},\n",
       "    {'tok_str': '11', 'prob': 0.030945507809519768},\n",
       "    {'tok_str': '2', 'prob': 0.0},\n",
       "    {'tok_str': '02', 'prob': 0.0},\n",
       "    {'tok_str': '05', 'prob': 0.0},\n",
       "    {'tok_str': '011', 'prob': 0.0},\n",
       "    {'tok_str': '001', 'prob': 0.0},\n",
       "    {'tok_str': '5', 'prob': 0.0},\n",
       "    {'tok_str': '10', 'prob': 0.0}]},\n",
       "  {'content': ' (',\n",
       "   'probs': [{'tok_str': ' (', 'prob': 0.9265420436859131},\n",
       "    {'tok_str': '\\n\\n', 'prob': 0.07345796376466751},\n",
       "    {'tok_str': ' =', 'prob': 0.0},\n",
       "    {'tok_str': '.\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': '\\n', 'prob': 0.0},\n",
       "    {'tok_str': ' +', 'prob': 0.0},\n",
       "    {'tok_str': ' \\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': '^', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': '.', 'prob': 0.0}]},\n",
       "  {'content': 'where',\n",
       "   'probs': [{'tok_str': 'where', 'prob': 0.9439119696617126},\n",
       "    {'tok_str': 'again', 'prob': 0.05608802288770676},\n",
       "    {'tok_str': 'adding', 'prob': 0.0},\n",
       "    {'tok_str': 'similar', 'prob': 0.0},\n",
       "    {'tok_str': 'since', 'prob': 0.0},\n",
       "    {'tok_str': 'the', 'prob': 0.0},\n",
       "    {'tok_str': 'three', 'prob': 0.0},\n",
       "    {'tok_str': 'with', 'prob': 0.0},\n",
       "    {'tok_str': 'same', 'prob': 0.0},\n",
       "    {'tok_str': 'two', 'prob': 0.0}]},\n",
       "  {'content': ' the',\n",
       "   'probs': [{'tok_str': ' the', 'prob': 1.0},\n",
       "    {'tok_str': ' +', 'prob': 0.0},\n",
       "    {'tok_str': ' again', 'prob': 0.0},\n",
       "    {'tok_str': ' ', 'prob': 0.0},\n",
       "    {'tok_str': ' we', 'prob': 0.0},\n",
       "    {'tok_str': ' another', 'prob': 0.0},\n",
       "    {'tok_str': ' each', 'prob': 0.0},\n",
       "    {'tok_str': ' two', 'prob': 0.0},\n",
       "    {'tok_str': ' three', 'prob': 0.0},\n",
       "    {'tok_str': ' there', 'prob': 0.0}]},\n",
       "  {'content': ' +',\n",
       "   'probs': [{'tok_str': ' +', 'prob': 1.0},\n",
       "    {'tok_str': ' three', 'prob': 0.0},\n",
       "    {'tok_str': ' ', 'prob': 0.0},\n",
       "    {'tok_str': ' ++', 'prob': 0.0},\n",
       "    {'tok_str': ' two', 'prob': 0.0},\n",
       "    {'tok_str': ' multiple', 'prob': 0.0},\n",
       "    {'tok_str': ' four', 'prob': 0.0},\n",
       "    {'tok_str': ' second', 'prob': 0.0},\n",
       "    {'tok_str': ' +=', 'prob': 0.0},\n",
       "    {'tok_str': ' additional', 'prob': 0.0}]},\n",
       "  {'content': ' sign',\n",
       "   'probs': [{'tok_str': ' sign', 'prob': 1.0},\n",
       "    {'tok_str': ' signs', 'prob': 0.0},\n",
       "    {'tok_str': ' +', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': ' ', 'prob': 0.0},\n",
       "    {'tok_str': ' symbol', 'prob': 0.0},\n",
       "    {'tok_str': '3', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': '5', 'prob': 0.0},\n",
       "    {'tok_str': ' again', 'prob': 0.0}]},\n",
       "  {'content': ' indicates',\n",
       "   'probs': [{'tok_str': ' indicates', 'prob': 1.0},\n",
       "    {'tok_str': ' again', 'prob': 0.0},\n",
       "    {'tok_str': ' is', 'prob': 0.0},\n",
       "    {'tok_str': ' also', 'prob': 0.0},\n",
       "    {'tok_str': ' repeats', 'prob': 0.0},\n",
       "    {'tok_str': ' represents', 'prob': 0.0},\n",
       "    {'tok_str': ' in', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': ' indicate', 'prob': 0.0},\n",
       "    {'tok_str': ' at', 'prob': 0.0}]},\n",
       "  {'content': ' adding',\n",
       "   'probs': [{'tok_str': ' adding', 'prob': 1.0},\n",
       "    {'tok_str': ' another', 'prob': 0.0},\n",
       "    {'tok_str': ' repeating', 'prob': 0.0},\n",
       "    {'tok_str': ' two', 'prob': 0.0},\n",
       "    {'tok_str': ' the', 'prob': 0.0},\n",
       "    {'tok_str': ' three', 'prob': 0.0},\n",
       "    {'tok_str': ' again', 'prob': 0.0},\n",
       "    {'tok_str': ' combining', 'prob': 0.0},\n",
       "    {'tok_str': ' a', 'prob': 0.0},\n",
       "    {'tok_str': ' addition', 'prob': 0.0}]},\n",
       "  {'content': ')\\n\\n',\n",
       "   'probs': [{'tok_str': ')\\n\\n', 'prob': 0.9748422503471375},\n",
       "    {'tok_str': ' again', 'prob': 0.025157831609249115},\n",
       "    {'tok_str': ' ', 'prob': 0.0},\n",
       "    {'tok_str': ' three', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': ' another', 'prob': 0.0},\n",
       "    {'tok_str': ' the', 'prob': 0.0},\n",
       "    {'tok_str': ' two', 'prob': 0.0},\n",
       "    {'tok_str': ' twice', 'prob': 0.0},\n",
       "    {'tok_str': ')', 'prob': 0.0}]},\n",
       "  {'content': 'Both',\n",
       "   'probs': [{'tok_str': 'Since', 'prob': 0.8172144889831543},\n",
       "    {'tok_str': 'Both', 'prob': 0.08213786780834198},\n",
       "    {'tok_str': 'Now', 'prob': 0.029970189556479454},\n",
       "    {'tok_str': 'Notice', 'prob': 0.024580450728535652},\n",
       "    {'tok_str': 'We', 'prob': 0.024328023195266724},\n",
       "    {'tok_str': 'In', 'prob': 0.02176905795931816},\n",
       "    {'tok_str': 'The', 'prob': 0.0},\n",
       "    {'tok_str': 'When', 'prob': 0.0},\n",
       "    {'tok_str': 'Compar', 'prob': 0.0},\n",
       "    {'tok_str': '9', 'prob': 0.0}]},\n",
       "  {'content': ' numbers',\n",
       "   'probs': [{'tok_str': ' numbers', 'prob': 0.8778596520423889},\n",
       "    {'tok_str': ' ', 'prob': 0.07593673467636108},\n",
       "    {'tok_str': ' expressions', 'prob': 0.025088226422667503},\n",
       "    {'tok_str': ' decimals', 'prob': 0.021115366369485855},\n",
       "    {'tok_str': ' decimal', 'prob': 0.0},\n",
       "    {'tok_str': ' values', 'prob': 0.0},\n",
       "    {'tok_str': ' are', 'prob': 0.0},\n",
       "    {'tok_str': ' have', 'prob': 0.0},\n",
       "    {'tok_str': ' expansions', 'prob': 0.0},\n",
       "    {'tok_str': ' representations', 'prob': 0.0}]},\n",
       "  {'content': ' are',\n",
       "   'probs': [{'tok_str': ' have', 'prob': 0.8086891770362854},\n",
       "    {'tok_str': ' are', 'prob': 0.19131086766719818},\n",
       "    {'tok_str': ' can', 'prob': 0.0},\n",
       "    {'tok_str': ' represent', 'prob': 0.0},\n",
       "    {'tok_str': ' end', 'prob': 0.0},\n",
       "    {'tok_str': ' add', 'prob': 0.0},\n",
       "    {'tok_str': ' ', 'prob': 0.0},\n",
       "    {'tok_str': ' start', 'prob': 0.0},\n",
       "    {'tok_str': ' contain', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0}]},\n",
       "  {'content': ' equal',\n",
       "   'probs': [{'tok_str': ' equal', 'prob': 0.899537205696106},\n",
       "    {'tok_str': ' the', 'prob': 0.05452689155936241},\n",
       "    {'tok_str': ' in', 'prob': 0.023748358711600304},\n",
       "    {'tok_str': ' very', 'prob': 0.02218751609325409},\n",
       "    {'tok_str': ' equivalent', 'prob': 0.0},\n",
       "    {'tok_str': ' ', 'prob': 0.0},\n",
       "    {'tok_str': ' essentially', 'prob': 0.0},\n",
       "    {'tok_str': ' identical', 'prob': 0.0},\n",
       "    {'tok_str': ' greater', 'prob': 0.0},\n",
       "    {'tok_str': ' less', 'prob': 0.0}]},\n",
       "  {'content': ' to',\n",
       "   'probs': [{'tok_str': ' to', 'prob': 0.5855543613433838},\n",
       "    {'tok_str': ',', 'prob': 0.38294804096221924},\n",
       "    {'tok_str': ' when', 'prob': 0.016225488856434822},\n",
       "    {'tok_str': '.', 'prob': 0.015272090211510658},\n",
       "    {'tok_str': ' in', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ':', 'prob': 0.0},\n",
       "    {'tok_str': ' except', 'prob': 0.0},\n",
       "    {'tok_str': ' because', 'prob': 0.0},\n",
       "    {'tok_str': ';', 'prob': 0.0}]},\n",
       "  {'content': ' ',\n",
       "   'probs': [{'tok_str': ' ', 'prob': 1.0},\n",
       "    {'tok_str': ' the', 'prob': 0.0},\n",
       "    {'tok_str': ' +', 'prob': 0.0},\n",
       "    {'tok_str': ' their', 'prob': 0.0},\n",
       "    {'tok_str': ' a', 'prob': 0.0},\n",
       "    {'tok_str': ' approximately', 'prob': 0.0},\n",
       "    {'tok_str': ' adding', 'prob': 0.0},\n",
       "    {'tok_str': ' \"', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' or', 'prob': 0.0}]},\n",
       "  {'content': '9',\n",
       "   'probs': [{'tok_str': '9', 'prob': 1.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': '0', 'prob': 0.0},\n",
       "    {'tok_str': '8', 'prob': 0.0},\n",
       "    {'tok_str': '11', 'prob': 0.0},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': '19', 'prob': 0.0},\n",
       "    {'tok_str': '99', 'prob': 0.0},\n",
       "    {'tok_str': '100', 'prob': 0.0},\n",
       "    {'tok_str': '4', 'prob': 0.0}]},\n",
       "  {'content': '.',\n",
       "   'probs': [{'tok_str': '.', 'prob': 0.5820127129554749},\n",
       "    {'tok_str': ' +', 'prob': 0.3397298753261566},\n",
       "    {'tok_str': ',', 'prob': 0.07825740426778793},\n",
       "    {'tok_str': ' with', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': ' because', 'prob': 0.0},\n",
       "    {'tok_str': ' plus', 'prob': 0.0},\n",
       "    {'tok_str': '.\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' when', 'prob': 0.0}]},\n",
       "  {'content': ' Since',\n",
       "   'probs': [{'tok_str': '0', 'prob': 0.2978847622871399},\n",
       "    {'tok_str': '1', 'prob': 0.26953303813934326},\n",
       "    {'tok_str': '9', 'prob': 0.2591955363750458},\n",
       "    {'tok_str': ' We', 'prob': 0.0228741317987442},\n",
       "    {'tok_str': '11', 'prob': 0.019326573237776756},\n",
       "    {'tok_str': '999', 'prob': 0.018831726163625717},\n",
       "    {'tok_str': ' This', 'prob': 0.017009185627102852},\n",
       "    {'tok_str': '01', 'prob': 0.014434543438255787},\n",
       "    {'tok_str': ' The', 'prob': 0.012473627924919128},\n",
       "    {'tok_str': ' In', 'prob': 0.01223266962915659}]},\n",
       "  {'content': ' ',\n",
       "   'probs': [{'tok_str': ' ', 'prob': 0.6078845262527466},\n",
       "    {'tok_str': ' they', 'prob': 0.1749218851327896},\n",
       "    {'tok_str': ' both', 'prob': 0.08630701899528503},\n",
       "    {'tok_str': ' the', 'prob': 0.07798720896244049},\n",
       "    {'tok_str': ' we', 'prob': 0.05289928615093231},\n",
       "    {'tok_str': ' neither', 'prob': 0.0},\n",
       "    {'tok_str': ' there', 'prob': 0.0},\n",
       "    {'tok_str': ' all', 'prob': 0.0},\n",
       "    {'tok_str': ' none', 'prob': 0.0},\n",
       "    {'tok_str': ' it', 'prob': 0.0}]},\n",
       "  {'content': '9',\n",
       "   'probs': [{'tok_str': '9', 'prob': 0.775615394115448},\n",
       "    {'tok_str': '0', 'prob': 0.22438456118106842},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': '8', 'prob': 0.0},\n",
       "    {'tok_str': '4', 'prob': 0.0},\n",
       "    {'tok_str': '3', 'prob': 0.0},\n",
       "    {'tok_str': '2', 'prob': 0.0},\n",
       "    {'tok_str': '5', 'prob': 0.0},\n",
       "    {'tok_str': '11', 'prob': 0.0}]},\n",
       "  {'content': ' is',\n",
       "   'probs': [{'tok_str': ' is', 'prob': 0.7760009169578552},\n",
       "    {'tok_str': '.', 'prob': 0.22399908304214478},\n",
       "    {'tok_str': ' <', 'prob': 0.0},\n",
       "    {'tok_str': ' >', 'prob': 0.0},\n",
       "    {'tok_str': ' +', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': ' has', 'prob': 0.0},\n",
       "    {'tok_str': ' =', 'prob': 0.0},\n",
       "    {'tok_str': ' -', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0}]},\n",
       "  {'content': ' less',\n",
       "   'probs': [{'tok_str': ' less', 'prob': 0.6478431820869446},\n",
       "    {'tok_str': ' greater', 'prob': 0.2696828842163086},\n",
       "    {'tok_str': ' smaller', 'prob': 0.0351080447435379},\n",
       "    {'tok_str': ' larger', 'prob': 0.03083447366952896},\n",
       "    {'tok_str': ' the', 'prob': 0.016531379893422127},\n",
       "    {'tok_str': ' a', 'prob': 0.0},\n",
       "    {'tok_str': ' closer', 'prob': 0.0},\n",
       "    {'tok_str': ' equal', 'prob': 0.0},\n",
       "    {'tok_str': ' exactly', 'prob': 0.0},\n",
       "    {'tok_str': ' an', 'prob': 0.0}]},\n",
       "  {'content': ' than',\n",
       "   'probs': [{'tok_str': ' than', 'prob': 1.0},\n",
       "    {'tok_str': ' equal', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': ' then', 'prob': 0.0},\n",
       "    {'tok_str': ' ', 'prob': 0.0},\n",
       "    {'tok_str': '_than', 'prob': 0.0},\n",
       "    {'tok_str': ' significant', 'prob': 0.0},\n",
       "    {'tok_str': '-than', 'prob': 0.0},\n",
       "    {'tok_str': ' Than', 'prob': 0.0},\n",
       "    {'tok_str': ' THAN', 'prob': 0.0}]},\n",
       "  {'content': ' ',\n",
       "   'probs': [{'tok_str': ' ', 'prob': 0.89110267162323},\n",
       "    {'tok_str': ' both', 'prob': 0.10889727622270584},\n",
       "    {'tok_str': ' or', 'prob': 0.0},\n",
       "    {'tok_str': ' any', 'prob': 0.0},\n",
       "    {'tok_str': ' the', 'prob': 0.0},\n",
       "    {'tok_str': ' all', 'prob': 0.0},\n",
       "    {'tok_str': ' its', 'prob': 0.0},\n",
       "    {'tok_str': ' either', 'prob': 0.0},\n",
       "    {'tok_str': ' each', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0}]},\n",
       "  {'content': '9',\n",
       "   'probs': [{'tok_str': '9', 'prob': 0.866395890712738},\n",
       "    {'tok_str': '10', 'prob': 0.13360409438610077},\n",
       "    {'tok_str': '11', 'prob': 0.0},\n",
       "    {'tok_str': '0', 'prob': 0.0},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': '12', 'prob': 0.0},\n",
       "    {'tok_str': '99', 'prob': 0.0},\n",
       "    {'tok_str': '90', 'prob': 0.0},\n",
       "    {'tok_str': '100', 'prob': 0.0},\n",
       "    {'tok_str': '19', 'prob': 0.0}]},\n",
       "  {'content': '.',\n",
       "   'probs': [{'tok_str': '.', 'prob': 1.0},\n",
       "    {'tok_str': ' +', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': '+', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' with', 'prob': 0.0},\n",
       "    {'tok_str': ' in', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': ' plus', 'prob': 0.0},\n",
       "    {'tok_str': ' or', 'prob': 0.0}]},\n",
       "  {'content': '1',\n",
       "   'probs': [{'tok_str': '1', 'prob': 0.8691117167472839},\n",
       "    {'tok_str': '11', 'prob': 0.13088828325271606},\n",
       "    {'tok_str': '9', 'prob': 0.0},\n",
       "    {'tok_str': '99', 'prob': 0.0},\n",
       "    {'tok_str': '0', 'prob': 0.0},\n",
       "    {'tok_str': '01', 'prob': 0.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': '09', 'prob': 0.0},\n",
       "    {'tok_str': '100', 'prob': 0.0},\n",
       "    {'tok_str': '111', 'prob': 0.0}]},\n",
       "  {'content': ',',\n",
       "   'probs': [{'tok_str': ',', 'prob': 1.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': ' is', 'prob': 0.0},\n",
       "    {'tok_str': ' in', 'prob': 0.0},\n",
       "    {'tok_str': ' or', 'prob': 0.0},\n",
       "    {'tok_str': '.', 'prob': 0.0},\n",
       "    {'tok_str': ' for', 'prob': 0.0},\n",
       "    {'tok_str': '<|eot_id|>', 'prob': 0.0},\n",
       "    {'tok_str': ' when', 'prob': 0.0}]},\n",
       "  {'content': ' ',\n",
       "   'probs': [{'tok_str': ' ', 'prob': 0.40704992413520813},\n",
       "    {'tok_str': ' we', 'prob': 0.40484634041786194},\n",
       "    {'tok_str': ' the', 'prob': 0.14149072766304016},\n",
       "    {'tok_str': ' I', 'prob': 0.03472945839166641},\n",
       "    {'tok_str': ' it', 'prob': 0.011883500963449478},\n",
       "    {'tok_str': ' therefore', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': ' which', 'prob': 0.0},\n",
       "    {'tok_str': ' both', 'prob': 0.0},\n",
       "    {'tok_str': ' our', 'prob': 0.0}]},\n",
       "  {'content': '9',\n",
       "   'probs': [{'tok_str': '9', 'prob': 1.0},\n",
       "    {'tok_str': '0', 'prob': 0.0},\n",
       "    {'tok_str': '8', 'prob': 0.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': '11', 'prob': 0.0},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': '7', 'prob': 0.0},\n",
       "    {'tok_str': '4', 'prob': 0.0},\n",
       "    {'tok_str': '09', 'prob': 0.0},\n",
       "    {'tok_str': '3', 'prob': 0.0}]},\n",
       "  {'content': '.',\n",
       "   'probs': [{'tok_str': '.', 'prob': 1.0},\n",
       "    {'tok_str': ' is', 'prob': 0.0},\n",
       "    {'tok_str': ' must', 'prob': 0.0},\n",
       "    {'tok_str': ' should', 'prob': 0.0},\n",
       "    {'tok_str': ' will', 'prob': 0.0},\n",
       "    {'tok_str': ' would', 'prob': 0.0},\n",
       "    {'tok_str': ' has', 'prob': 0.0},\n",
       "    {'tok_str': ' cannot', 'prob': 0.0},\n",
       "    {'tok_str': ' can', 'prob': 0.0},\n",
       "    {'tok_str': ' +', 'prob': 0.0}]},\n",
       "  {'content': '9',\n",
       "   'probs': [{'tok_str': '11', 'prob': 0.7793388962745667},\n",
       "    {'tok_str': '9', 'prob': 0.22066110372543335},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': '99', 'prob': 0.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': '111', 'prob': 0.0},\n",
       "    {'tok_str': '8', 'prob': 0.0},\n",
       "    {'tok_str': '121', 'prob': 0.0},\n",
       "    {'tok_str': '100', 'prob': 0.0},\n",
       "    {'tok_str': '12', 'prob': 0.0}]},\n",
       "  {'content': ' is',\n",
       "   'probs': [{'tok_str': ' is', 'prob': 1.0},\n",
       "    {'tok_str': ' must', 'prob': 0.0},\n",
       "    {'tok_str': ' will', 'prob': 0.0},\n",
       "    {'tok_str': ' has', 'prob': 0.0},\n",
       "    {'tok_str': ' should', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' >', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': ' <', 'prob': 0.0},\n",
       "    {'tok_str': ' would', 'prob': 0.0}]},\n",
       "  {'content': ' also',\n",
       "   'probs': [{'tok_str': ' larger', 'prob': 0.39116016030311584},\n",
       "    {'tok_str': ' less', 'prob': 0.3432077467441559},\n",
       "    {'tok_str': ' smaller', 'prob': 0.10664352029561996},\n",
       "    {'tok_str': ' actually', 'prob': 0.07818653434515},\n",
       "    {'tok_str': ' greater', 'prob': 0.06167887523770332},\n",
       "    {'tok_str': ' also', 'prob': 0.019123192876577377},\n",
       "    {'tok_str': ' indeed', 'prob': 0.0},\n",
       "    {'tok_str': ' the', 'prob': 0.0},\n",
       "    {'tok_str': ' not', 'prob': 0.0},\n",
       "    {'tok_str': ' definitely', 'prob': 0.0}]},\n",
       "  {'content': ' less',\n",
       "   'probs': [{'tok_str': ' less', 'prob': 1.0},\n",
       "    {'tok_str': ' larger', 'prob': 0.0},\n",
       "    {'tok_str': ' greater', 'prob': 0.0},\n",
       "    {'tok_str': ' smaller', 'prob': 0.0},\n",
       "    {'tok_str': ' equal', 'prob': 0.0},\n",
       "    {'tok_str': ' slightly', 'prob': 0.0},\n",
       "    {'tok_str': ' the', 'prob': 0.0},\n",
       "    {'tok_str': ' ', 'prob': 0.0},\n",
       "    {'tok_str': ' lesser', 'prob': 0.0},\n",
       "    {'tok_str': ' more', 'prob': 0.0}]},\n",
       "  {'content': ' than',\n",
       "   'probs': [{'tok_str': ' than', 'prob': 1.0},\n",
       "    {'tok_str': '.', 'prob': 0.0},\n",
       "    {'tok_str': '.\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': ' equal', 'prob': 0.0},\n",
       "    {'tok_str': ' in', 'prob': 0.0},\n",
       "    {'tok_str': ' Than', 'prob': 0.0},\n",
       "    {'tok_str': '_than', 'prob': 0.0},\n",
       "    {'tok_str': ' compared', 'prob': 0.0},\n",
       "    {'tok_str': ' that', 'prob': 0.0}]},\n",
       "  {'content': ' ',\n",
       "   'probs': [{'tok_str': ' ', 'prob': 1.0},\n",
       "    {'tok_str': ' or', 'prob': 0.0},\n",
       "    {'tok_str': ' both', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': ' the', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' but', 'prob': 0.0},\n",
       "    {'tok_str': '...', 'prob': 0.0},\n",
       "    {'tok_str': ' either', 'prob': 0.0}]},\n",
       "  {'content': '9',\n",
       "   'probs': [{'tok_str': '9', 'prob': 1.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': ' ', 'prob': 0.0},\n",
       "    {'tok_str': '0', 'prob': 0.0},\n",
       "    {'tok_str': '8', 'prob': 0.0},\n",
       "    {'tok_str': '11', 'prob': 0.0},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': '09', 'prob': 0.0},\n",
       "    {'tok_str': '4', 'prob': 0.0},\n",
       "    {'tok_str': '99', 'prob': 0.0}]},\n",
       "  {'content': '.',\n",
       "   'probs': [{'tok_str': '.', 'prob': 1.0},\n",
       "    {'tok_str': '.\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': ' +', 'prob': 0.0},\n",
       "    {'tok_str': ' is', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' or', 'prob': 0.0},\n",
       "    {'tok_str': '.x', 'prob': 0.0},\n",
       "    {'tok_str': '\\n\\n', 'prob': 0.0}]},\n",
       "  {'content': '11',\n",
       "   'probs': [{'tok_str': '11', 'prob': 1.0},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': ' The', 'prob': 0.0},\n",
       "    {'tok_str': '0', 'prob': 0.0},\n",
       "    {'tok_str': ' We', 'prob': 0.0},\n",
       "    {'tok_str': '111', 'prob': 0.0},\n",
       "    {'tok_str': ' \\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': '9', 'prob': 0.0},\n",
       "    {'tok_str': '12', 'prob': 0.0}]},\n",
       "  {'content': ',',\n",
       "   'probs': [{'tok_str': '.\\n\\n', 'prob': 0.4789113402366638},\n",
       "    {'tok_str': '.', 'prob': 0.4619554579257965},\n",
       "    {'tok_str': ',', 'prob': 0.05913322791457176},\n",
       "    {'tok_str': ' in', 'prob': 0.0},\n",
       "    {'tok_str': '<|eot_id|>', 'prob': 0.0},\n",
       "    {'tok_str': '\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': ' because', 'prob': 0.0},\n",
       "    {'tok_str': ' Therefore', 'prob': 0.0}]},\n",
       "  {'content': ' and',\n",
       "   'probs': [{'tok_str': ' and', 'prob': 0.543134331703186},\n",
       "    {'tok_str': ' so', 'prob': 0.16772329807281494},\n",
       "    {'tok_str': ' making', 'prob': 0.13787466287612915},\n",
       "    {'tok_str': ' therefore', 'prob': 0.10686609894037247},\n",
       "    {'tok_str': ' which', 'prob': 0.04440157487988472},\n",
       "    {'tok_str': ' but', 'prob': 0.0},\n",
       "    {'tok_str': ' we', 'prob': 0.0},\n",
       "    {'tok_str': ' I', 'prob': 0.0},\n",
       "    {'tok_str': ' the', 'prob': 0.0},\n",
       "    {'tok_str': ' thus', 'prob': 0.0}]},\n",
       "  {'content': ' therefore',\n",
       "   'probs': [{'tok_str': ' therefore', 'prob': 1.0},\n",
       "    {'tok_str': ' thus', 'prob': 0.0},\n",
       "    {'tok_str': ' ', 'prob': 0.0},\n",
       "    {'tok_str': ' the', 'prob': 0.0},\n",
       "    {'tok_str': ' hence', 'prob': 0.0},\n",
       "    {'tok_str': ' so', 'prob': 0.0},\n",
       "    {'tok_str': ' consequently', 'prob': 0.0},\n",
       "    {'tok_str': ' both', 'prob': 0.0},\n",
       "    {'tok_str': ' it', 'prob': 0.0},\n",
       "    {'tok_str': ' we', 'prob': 0.0}]},\n",
       "  {'content': ' ',\n",
       "   'probs': [{'tok_str': ' ', 'prob': 0.5704163908958435},\n",
       "    {'tok_str': ',', 'prob': 0.4295836091041565},\n",
       "    {'tok_str': ' the', 'prob': 0.0},\n",
       "    {'tok_str': ':\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': ' less', 'prob': 0.0},\n",
       "    {'tok_str': '...', 'prob': 0.0},\n",
       "    {'tok_str': '...\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': ' greater', 'prob': 0.0},\n",
       "    {'tok_str': ' it', 'prob': 0.0},\n",
       "    {'tok_str': ' both', 'prob': 0.0}]},\n",
       "  {'content': '9',\n",
       "   'probs': [{'tok_str': '9', 'prob': 1.0},\n",
       "    {'tok_str': '0', 'prob': 0.0},\n",
       "    {'tok_str': '8', 'prob': 0.0},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': '09', 'prob': 0.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': '7', 'prob': 0.0},\n",
       "    {'tok_str': '4', 'prob': 0.0},\n",
       "    {'tok_str': '3', 'prob': 0.0},\n",
       "    {'tok_str': '11', 'prob': 0.0}]},\n",
       "  {'content': '.',\n",
       "   'probs': [{'tok_str': '.', 'prob': 1.0},\n",
       "    {'tok_str': ' is', 'prob': 0.0},\n",
       "    {'tok_str': '.\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': ' >', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' has', 'prob': 0.0},\n",
       "    {'tok_str': ' +', 'prob': 0.0},\n",
       "    {'tok_str': ' will', 'prob': 0.0},\n",
       "    {'tok_str': ' would', 'prob': 0.0}]},\n",
       "  {'content': '9',\n",
       "   'probs': [{'tok_str': '9', 'prob': 0.9618664979934692},\n",
       "    {'tok_str': '11', 'prob': 0.03813354671001434},\n",
       "    {'tok_str': '99', 'prob': 0.0},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': '8', 'prob': 0.0},\n",
       "    {'tok_str': '111', 'prob': 0.0},\n",
       "    {'tok_str': '100', 'prob': 0.0},\n",
       "    {'tok_str': '91', 'prob': 0.0},\n",
       "    {'tok_str': ' \\n\\n', 'prob': 0.0}]},\n",
       "  {'content': ' is',\n",
       "   'probs': [{'tok_str': ' is', 'prob': 1.0},\n",
       "    {'tok_str': ' <', 'prob': 0.0},\n",
       "    {'tok_str': ' >', 'prob': 0.0},\n",
       "    {'tok_str': ' has', 'prob': 0.0},\n",
       "    {'tok_str': ' will', 'prob': 0.0},\n",
       "    {'tok_str': ' must', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': '.\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': ' ≤', 'prob': 0.0}]},\n",
       "  {'content': ' larger',\n",
       "   'probs': [{'tok_str': ' larger', 'prob': 0.3578488826751709},\n",
       "    {'tok_str': ' smaller', 'prob': 0.3506537675857544},\n",
       "    {'tok_str': ' less', 'prob': 0.20916979014873505},\n",
       "    {'tok_str': ' the', 'prob': 0.08232761174440384},\n",
       "    {'tok_str': ' greater', 'prob': 0.0},\n",
       "    {'tok_str': ' not', 'prob': 0.0},\n",
       "    {'tok_str': ' actually', 'prob': 0.0},\n",
       "    {'tok_str': ' a', 'prob': 0.0},\n",
       "    {'tok_str': ' ', 'prob': 0.0},\n",
       "    {'tok_str': ' closer', 'prob': 0.0}]},\n",
       "  {'content': ' than',\n",
       "   'probs': [{'tok_str': ' than', 'prob': 0.8048030138015747},\n",
       "    {'tok_str': '.\\n\\n', 'prob': 0.1652248650789261},\n",
       "    {'tok_str': '.', 'prob': 0.02997209131717682},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': ' or', 'prob': 0.0},\n",
       "    {'tok_str': ' compared', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' in', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': '.\\n', 'prob': 0.0}]},\n",
       "  {'content': ' ',\n",
       "   'probs': [{'tok_str': ' ', 'prob': 1.0},\n",
       "    {'tok_str': ' both', 'prob': 0.0},\n",
       "    {'tok_str': ' or', 'prob': 0.0},\n",
       "    {'tok_str': ' the', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': '.\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': ' its', 'prob': 0.0},\n",
       "    {'tok_str': ' is', 'prob': 0.0}]},\n",
       "  {'content': '9',\n",
       "   'probs': [{'tok_str': '9', 'prob': 1.0},\n",
       "    {'tok_str': '0', 'prob': 0.0},\n",
       "    {'tok_str': '8', 'prob': 0.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': ' ', 'prob': 0.0},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': '4', 'prob': 0.0},\n",
       "    {'tok_str': '09', 'prob': 0.0},\n",
       "    {'tok_str': '3', 'prob': 0.0},\n",
       "    {'tok_str': '5', 'prob': 0.0}]},\n",
       "  {'content': '.',\n",
       "   'probs': [{'tok_str': '.', 'prob': 1.0},\n",
       "    {'tok_str': '.\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': ' is', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': '.\\n', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ':', 'prob': 0.0},\n",
       "    {'tok_str': ' or', 'prob': 0.0},\n",
       "    {'tok_str': ' with', 'prob': 0.0}]},\n",
       "  {'content': '11',\n",
       "   'probs': [{'tok_str': '11', 'prob': 1.0},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': '111', 'prob': 0.0},\n",
       "    {'tok_str': '12', 'prob': 0.0},\n",
       "    {'tok_str': '121', 'prob': 0.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': '011', 'prob': 0.0},\n",
       "    {'tok_str': ' ', 'prob': 0.0},\n",
       "    {'tok_str': '99', 'prob': 0.0},\n",
       "    {'tok_str': ' \\n\\n', 'prob': 0.0}]},\n",
       "  {'content': '.\\n\\n',\n",
       "   'probs': [{'tok_str': '.\\n\\n', 'prob': 0.7514597177505493},\n",
       "    {'tok_str': '.', 'prob': 0.2485402375459671},\n",
       "    {'tok_str': '<|eot_id|>', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': '.\\n\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': '.\\n', 'prob': 0.0},\n",
       "    {'tok_str': ' in', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': ' is', 'prob': 0.0}]},\n",
       "  {'content': '<',\n",
       "   'probs': [{'tok_str': 'Reflection', 'prob': 0.3774150013923645},\n",
       "    {'tok_str': '<', 'prob': 0.22192330658435822},\n",
       "    {'tok_str': 'However', 'prob': 0.09112649410963058},\n",
       "    {'tok_str': 'Here', 'prob': 0.08943983167409897},\n",
       "    {'tok_str': 'I', 'prob': 0.050362568348646164},\n",
       "    {'tok_str': '**', 'prob': 0.04987732693552971},\n",
       "    {'tok_str': 'The', 'prob': 0.020826095715165138},\n",
       "    {'tok_str': 'My', 'prob': 0.01703323796391487},\n",
       "    {'tok_str': 'Let', 'prob': 0.016529187560081482},\n",
       "    {'tok_str': 'In', 'prob': 0.016199128702282906}]},\n",
       "  {'content': 'reflection',\n",
       "   'probs': [{'tok_str': 'reflection', 'prob': 0.8905099034309387},\n",
       "    {'tok_str': 'output', 'prob': 0.1094900444149971},\n",
       "    {'tok_str': 'Reflection', 'prob': 0.0},\n",
       "    {'tok_str': 'ref', 'prob': 0.0},\n",
       "    {'tok_str': 'Output', 'prob': 0.0},\n",
       "    {'tok_str': 'reflect', 'prob': 0.0},\n",
       "    {'tok_str': 'thought', 'prob': 0.0},\n",
       "    {'tok_str': ' Reflection', 'prob': 0.0},\n",
       "    {'tok_str': 'thinking', 'prob': 0.0},\n",
       "    {'tok_str': 'out', 'prob': 0.0}]},\n",
       "  {'content': '>',\n",
       "   'probs': [{'tok_str': '>', 'prob': 0.9353905916213989},\n",
       "    {'tok_str': '>\\n', 'prob': 0.038756996393203735},\n",
       "    {'tok_str': '>\\n\\n', 'prob': 0.025852417573332787},\n",
       "    {'tok_str': '>:', 'prob': 0.0},\n",
       "    {'tok_str': '>This', 'prob': 0.0},\n",
       "    {'tok_str': '>The', 'prob': 0.0},\n",
       "    {'tok_str': '>I', 'prob': 0.0},\n",
       "    {'tok_str': '>No', 'prob': 0.0},\n",
       "    {'tok_str': ':>', 'prob': 0.0},\n",
       "    {'tok_str': '>R', 'prob': 0.0}]},\n",
       "  {'content': ' I',\n",
       "   'probs': [{'tok_str': ' I', 'prob': 0.34672072529792786},\n",
       "    {'tok_str': 'Actually', 'prob': 0.23062771558761597},\n",
       "    {'tok_str': ' Wait', 'prob': 0.146145299077034},\n",
       "    {'tok_str': ' Ah', 'prob': 0.05925925076007843},\n",
       "    {'tok_str': ' \\n\\n', 'prob': 0.043360039591789246},\n",
       "    {'tok_str': 'Wait', 'prob': 0.03943340852856636},\n",
       "    {'tok_str': 'However', 'prob': 0.02759014070034027},\n",
       "    {'tok_str': ' Actually', 'prob': 0.023637795820832253},\n",
       "    {'tok_str': 'Upon', 'prob': 0.019274877384305},\n",
       "    {'tok_str': ' \\n', 'prob': 0.015258796513080597}]},\n",
       "  {'content': ' initially',\n",
       "   'probs': [{'tok_str': ' made', 'prob': 0.43438753485679626},\n",
       "    {'tok_str': ' initially', 'prob': 0.1942286342382431},\n",
       "    {'tok_str': ' realized', 'prob': 0.15519791841506958},\n",
       "    {'tok_str': ' should', 'prob': 0.05831367149949074},\n",
       "    {'tok_str': ' realize', 'prob': 0.04860658943653107},\n",
       "    {'tok_str': ' was', 'prob': 0.03681229054927826},\n",
       "    {'tok_str': ' noticed', 'prob': 0.020175009965896606},\n",
       "    {'tok_str': ' think', 'prob': 0.017081860452890396},\n",
       "    {'tok_str': ' see', 'prob': 0.01296508964151144},\n",
       "    {'tok_str': ' didn', 'prob': 0.011706013232469559}]},\n",
       "  {'content': ' considered',\n",
       "   'probs': [{'tok_str': ' thought', 'prob': 0.46633124351501465},\n",
       "    {'tok_str': ' considered', 'prob': 0.1541750282049179},\n",
       "    {'tok_str': ' compared', 'prob': 0.10476870089769363},\n",
       "    {'tok_str': ' wrote', 'prob': 0.0992422103881836},\n",
       "    {'tok_str': ' concluded', 'prob': 0.08037737756967545},\n",
       "    {'tok_str': ' assumed', 'prob': 0.052003439515829086},\n",
       "    {'tok_str': ' said', 'prob': 0.016605041921138763},\n",
       "    {'tok_str': ' made', 'prob': 0.013455357402563095},\n",
       "    {'tok_str': ' incorrectly', 'prob': 0.01304160337895155},\n",
       "    {'tok_str': ' mis', 'prob': 0.0}]},\n",
       "  {'content': ' comparing',\n",
       "   'probs': [{'tok_str': ' ', 'prob': 0.33268362283706665},\n",
       "    {'tok_str': ' the', 'prob': 0.3096436858177185},\n",
       "    {'tok_str': ' comparing', 'prob': 0.11042686551809311},\n",
       "    {'tok_str': ' adding', 'prob': 0.08945351839065552},\n",
       "    {'tok_str': ' that', 'prob': 0.06792206317186356},\n",
       "    {'tok_str': ' only', 'prob': 0.030035272240638733},\n",
       "    {'tok_str': ' using', 'prob': 0.02225383371114731},\n",
       "    {'tok_str': ' converting', 'prob': 0.016383429989218712},\n",
       "    {'tok_str': ' rounding', 'prob': 0.01102801039814949},\n",
       "    {'tok_str': ' both', 'prob': 0.010169715620577335}]},\n",
       "  {'content': ' the',\n",
       "   'probs': [{'tok_str': ' the', 'prob': 0.9660942554473877},\n",
       "    {'tok_str': ' ', 'prob': 0.03390570729970932},\n",
       "    {'tok_str': ' their', 'prob': 0.0},\n",
       "    {'tok_str': ' them', 'prob': 0.0},\n",
       "    {'tok_str': ' just', 'prob': 0.0},\n",
       "    {'tok_str': ' only', 'prob': 0.0},\n",
       "    {'tok_str': ' these', 'prob': 0.0},\n",
       "    {'tok_str': ' decimal', 'prob': 0.0},\n",
       "    {'tok_str': ' each', 'prob': 0.0},\n",
       "    {'tok_str': ' both', 'prob': 0.0}]},\n",
       "  {'content': ' numbers',\n",
       "   'probs': [{'tok_str': ' numbers', 'prob': 0.9124771952629089},\n",
       "    {'tok_str': ' whole', 'prob': 0.052114762365818024},\n",
       "    {'tok_str': ' two', 'prob': 0.035408053547143936},\n",
       "    {'tok_str': ' absolute', 'prob': 0.0},\n",
       "    {'tok_str': ' decimal', 'prob': 0.0},\n",
       "    {'tok_str': ' fractions', 'prob': 0.0},\n",
       "    {'tok_str': ' digits', 'prob': 0.0},\n",
       "    {'tok_str': ' values', 'prob': 0.0},\n",
       "    {'tok_str': ' decimals', 'prob': 0.0},\n",
       "    {'tok_str': ' fractional', 'prob': 0.0}]},\n",
       "  {'content': ' as',\n",
       "   'probs': [{'tok_str': ' as', 'prob': 0.730911135673523},\n",
       "    {'tok_str': ' directly', 'prob': 0.12327540665864944},\n",
       "    {'tok_str': ' by', 'prob': 0.05083772540092468},\n",
       "    {'tok_str': ' numer', 'prob': 0.034252915531396866},\n",
       "    {'tok_str': ' using', 'prob': 0.033258263021707535},\n",
       "    {'tok_str': ' in', 'prob': 0.027464624494314194},\n",
       "    {'tok_str': ' without', 'prob': 0.0},\n",
       "    {'tok_str': ' separately', 'prob': 0.0},\n",
       "    {'tok_str': ' to', 'prob': 0.0},\n",
       "    {'tok_str': ' based', 'prob': 0.0}]},\n",
       "  {'content': ' decimals',\n",
       "   'probs': [{'tok_str': ' decimals', 'prob': 0.35067346692085266},\n",
       "    {'tok_str': ' if', 'prob': 0.32786765694618225},\n",
       "    {'tok_str': ' is', 'prob': 0.09123104065656662},\n",
       "    {'tok_str': ' decimal', 'prob': 0.0835661068558693},\n",
       "    {'tok_str': ' whole', 'prob': 0.043084628880023956},\n",
       "    {'tok_str': ' they', 'prob': 0.02498026378452778},\n",
       "    {'tok_str': ' integers', 'prob': 0.024787236005067825},\n",
       "    {'tok_str': ' a', 'prob': 0.014659476466476917},\n",
       "    {'tok_str': ' separate', 'prob': 0.011851086281239986},\n",
       "    {'tok_str': ' strings', 'prob': 0.00995400920510292}]},\n",
       "  {'content': ',',\n",
       "   'probs': [{'tok_str': ',', 'prob': 0.9718884229660034},\n",
       "    {'tok_str': ' and', 'prob': 0.02811160869896412},\n",
       "    {'tok_str': ' without', 'prob': 0.0},\n",
       "    {'tok_str': ' by', 'prob': 0.0},\n",
       "    {'tok_str': ' instead', 'prob': 0.0},\n",
       "    {'tok_str': '.', 'prob': 0.0},\n",
       "    {'tok_str': ' with', 'prob': 0.0},\n",
       "    {'tok_str': ' to', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' directly', 'prob': 0.0}]},\n",
       "  {'content': ' but',\n",
       "   'probs': [{'tok_str': ' but', 'prob': 0.9725684523582458},\n",
       "    {'tok_str': ' which', 'prob': 0.027431543916463852},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': ' thinking', 'prob': 0.0},\n",
       "    {'tok_str': ' assuming', 'prob': 0.0},\n",
       "    {'tok_str': ' where', 'prob': 0.0},\n",
       "    {'tok_str': ' ignoring', 'prob': 0.0},\n",
       "    {'tok_str': ' as', 'prob': 0.0},\n",
       "    {'tok_str': ' then', 'prob': 0.0},\n",
       "    {'tok_str': ' not', 'prob': 0.0}]},\n",
       "  {'content': ' that',\n",
       "   'probs': [{'tok_str': ' I', 'prob': 0.4772842824459076},\n",
       "    {'tok_str': ' upon', 'prob': 0.16365298628807068},\n",
       "    {'tok_str': ' then', 'prob': 0.14006875455379486},\n",
       "    {'tok_str': ' since', 'prob': 0.05586213245987892},\n",
       "    {'tok_str': ' it', 'prob': 0.04465532675385475},\n",
       "    {'tok_str': ' the', 'prob': 0.03064257837831974},\n",
       "    {'tok_str': ' that', 'prob': 0.029350262135267258},\n",
       "    {'tok_str': ' in', 'prob': 0.02492811530828476},\n",
       "    {'tok_str': ' a', 'prob': 0.021695338189601898},\n",
       "    {'tok_str': ' realized', 'prob': 0.011860274709761143}]},\n",
       "  {'content': ' led',\n",
       "   'probs': [{'tok_str': ' was', 'prob': 0.3229005038738251},\n",
       "    {'tok_str': ' led', 'prob': 0.2808845639228821},\n",
       "    {'tok_str': \"'s\", 'prob': 0.1587456464767456},\n",
       "    {'tok_str': ' would', 'prob': 0.13222470879554749},\n",
       "    {'tok_str': ' didn', 'prob': 0.057675085961818695},\n",
       "    {'tok_str': ' wasn', 'prob': 0.01824590377509594},\n",
       "    {'tok_str': ' might', 'prob': 0.01109172124415636},\n",
       "    {'tok_str': ' doesn', 'prob': 0.01033227238804102},\n",
       "    {'tok_str': ' wouldn', 'prob': 0.00789962150156498},\n",
       "    {'tok_str': ' approach', 'prob': 0.0}]},\n",
       "  {'content': ' to',\n",
       "   'probs': [{'tok_str': ' to', 'prob': 0.7111583352088928},\n",
       "    {'tok_str': ' me', 'prob': 0.28884169459342957},\n",
       "    {'tok_str': ' directly', 'prob': 0.0},\n",
       "    {'tok_str': ' down', 'prob': 0.0},\n",
       "    {'tok_str': ' us', 'prob': 0.0},\n",
       "    {'tok_str': ' only', 'prob': 0.0},\n",
       "    {'tok_str': ' my', 'prob': 0.0},\n",
       "    {'tok_str': ' the', 'prob': 0.0},\n",
       "    {'tok_str': ' astr', 'prob': 0.0},\n",
       "    {'tok_str': ' a', 'prob': 0.0}]},\n",
       "  {'content': ' a',\n",
       "   'probs': [{'tok_str': ' a', 'prob': 0.561649739742279},\n",
       "    {'tok_str': ' an', 'prob': 0.3571547567844391},\n",
       "    {'tok_str': ' the', 'prob': 0.04177121818065643},\n",
       "    {'tok_str': ' incorrect', 'prob': 0.025952469557523727},\n",
       "    {'tok_str': ' unnecessary', 'prob': 0.013471772894263268},\n",
       "    {'tok_str': ' confusion', 'prob': 0.0},\n",
       "    {'tok_str': ' comparing', 'prob': 0.0},\n",
       "    {'tok_str': ' me', 'prob': 0.0},\n",
       "    {'tok_str': ' some', 'prob': 0.0},\n",
       "    {'tok_str': ' my', 'prob': 0.0}]},\n",
       "  {'content': ' different',\n",
       "   'probs': [{'tok_str': ' comparison', 'prob': 0.245647594332695},\n",
       "    {'tok_str': ' mistake', 'prob': 0.23633389174938202},\n",
       "    {'tok_str': ' misunderstanding', 'prob': 0.18635950982570648},\n",
       "    {'tok_str': ' straightforward', 'prob': 0.047776803374290466},\n",
       "    {'tok_str': ' simple', 'prob': 0.04489611089229584},\n",
       "    {'tok_str': ' wrong', 'prob': 0.02895224094390869},\n",
       "    {'tok_str': ' false', 'prob': 0.025163885205984116},\n",
       "    {'tok_str': ' m', 'prob': 0.02511521615087986},\n",
       "    {'tok_str': ' different', 'prob': 0.02347053587436676},\n",
       "    {'tok_str': ' logical', 'prob': 0.023266753181815147}]},\n",
       "  {'content': ' conclusion',\n",
       "   'probs': [{'tok_str': ' conclusion', 'prob': 0.5115551948547363},\n",
       "    {'tok_str': ' comparison', 'prob': 0.2360232174396515},\n",
       "    {'tok_str': ' answer', 'prob': 0.12573674321174622},\n",
       "    {'tok_str': ' result', 'prob': 0.09782680869102478},\n",
       "    {'tok_str': ' outcome', 'prob': 0.028858095407485962},\n",
       "    {'tok_str': ' interpretation', 'prob': 0.0},\n",
       "    {'tok_str': ' analysis', 'prob': 0.0},\n",
       "    {'tok_str': ' solution', 'prob': 0.0},\n",
       "    {'tok_str': ' calculation', 'prob': 0.0},\n",
       "    {'tok_str': ' ordering', 'prob': 0.0}]},\n",
       "  {'content': '.',\n",
       "   'probs': [{'tok_str': '.', 'prob': 0.8181014060974121},\n",
       "    {'tok_str': ' than', 'prob': 0.13449618220329285},\n",
       "    {'tok_str': ',', 'prob': 0.04740244895219803},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' that', 'prob': 0.0},\n",
       "    {'tok_str': ' based', 'prob': 0.0},\n",
       "    {'tok_str': ' due', 'prob': 0.0},\n",
       "    {'tok_str': ' -', 'prob': 0.0},\n",
       "    {'tok_str': '!', 'prob': 0.0},\n",
       "    {'tok_str': ';', 'prob': 0.0}]},\n",
       "  {'content': ' In',\n",
       "   'probs': [{'tok_str': ' A', 'prob': 0.24859583377838135},\n",
       "    {'tok_str': ' Let', 'prob': 0.12725351750850677},\n",
       "    {'tok_str': ' I', 'prob': 0.11667464673519135},\n",
       "    {'tok_str': ' The', 'prob': 0.10301745682954788},\n",
       "    {'tok_str': ' In', 'prob': 0.09813546389341354},\n",
       "    {'tok_str': ' To', 'prob': 0.06455761194229126},\n",
       "    {'tok_str': ' Upon', 'prob': 0.05662047117948532},\n",
       "    {'tok_str': ' It', 'prob': 0.04808168113231659},\n",
       "    {'tok_str': ' However', 'prob': 0.04110518842935562},\n",
       "    {'tok_str': ' This', 'prob': 0.028463611379265785}]},\n",
       "  {'content': ' reality',\n",
       "   'probs': [{'tok_str': ' reality', 'prob': 0.7387455701828003},\n",
       "    {'tok_str': ' this', 'prob': 0.13700443506240845},\n",
       "    {'tok_str': ' fact', 'prob': 0.06172600015997887},\n",
       "    {'tok_str': ' hindsight', 'prob': 0.03727404773235321},\n",
       "    {'tok_str': ' the', 'prob': 0.02524990774691105},\n",
       "    {'tok_str': ' a', 'prob': 0.0},\n",
       "    {'tok_str': ' retrospect', 'prob': 0.0},\n",
       "    {'tok_str': ' that', 'prob': 0.0},\n",
       "    {'tok_str': ' actual', 'prob': 0.0},\n",
       "    {'tok_str': ' contrast', 'prob': 0.0}]},\n",
       "  {'content': ',',\n",
       "   'probs': [{'tok_str': ',', 'prob': 1.0},\n",
       "    {'tok_str': ' I', 'prob': 0.0},\n",
       "    {'tok_str': ' both', 'prob': 0.0},\n",
       "    {'tok_str': ' ', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' we', 'prob': 0.0},\n",
       "    {'tok_str': ' the', 'prob': 0.0},\n",
       "    {'tok_str': ' ,', 'prob': 0.0},\n",
       "    {'tok_str': ' all', 'prob': 0.0},\n",
       "    {'tok_str': ' is', 'prob': 0.0}]},\n",
       "  {'content': ' we',\n",
       "   'probs': [{'tok_str': ' we', 'prob': 0.25998520851135254},\n",
       "    {'tok_str': ' the', 'prob': 0.2204522341489792},\n",
       "    {'tok_str': ' I', 'prob': 0.20965562760829926},\n",
       "    {'tok_str': ' it', 'prob': 0.12105710804462433},\n",
       "    {'tok_str': ' ', 'prob': 0.09122945368289948},\n",
       "    {'tok_str': ' both', 'prob': 0.05466388538479805},\n",
       "    {'tok_str': ' comparing', 'prob': 0.02170480042695999},\n",
       "    {'tok_str': ' a', 'prob': 0.011109010316431522},\n",
       "    {'tok_str': ' when', 'prob': 0.010142669081687927},\n",
       "    {'tok_str': ' adding', 'prob': 0.0}]},\n",
       "  {'content': ' should',\n",
       "   'probs': [{'tok_str': ' should', 'prob': 0.4866066873073578},\n",
       "    {'tok_str': ' need', 'prob': 0.24147476255893707},\n",
       "    {'tok_str': ' can', 'prob': 0.23516134917736053},\n",
       "    {'tok_str': \"'re\", 'prob': 0.012887861579656601},\n",
       "    {'tok_str': ' could', 'prob': 0.012025371193885803},\n",
       "    {'tok_str': ' are', 'prob': 0.011843971908092499},\n",
       "    {'tok_str': ' only', 'prob': 0.0},\n",
       "    {'tok_str': ' would', 'prob': 0.0},\n",
       "    {'tok_str': ' were', 'prob': 0.0},\n",
       "    {'tok_str': ' compare', 'prob': 0.0}]},\n",
       "  {'content': ' have',\n",
       "   'probs': [{'tok_str': ' have', 'prob': 0.4669833481311798},\n",
       "    {'tok_str': ' compare', 'prob': 0.3265950679779053},\n",
       "    {'tok_str': ' be', 'prob': 0.15503215789794922},\n",
       "    {'tok_str': ' consider', 'prob': 0.03684977814555168},\n",
       "    {'tok_str': ' also', 'prob': 0.014539518393576145},\n",
       "    {'tok_str': ' only', 'prob': 0.0},\n",
       "    {'tok_str': ' focus', 'prob': 0.0},\n",
       "    {'tok_str': ' convert', 'prob': 0.0},\n",
       "    {'tok_str': ' directly', 'prob': 0.0},\n",
       "    {'tok_str': ' still', 'prob': 0.0}]},\n",
       "  {'content': ' compared',\n",
       "   'probs': [{'tok_str': ' compared', 'prob': 0.6080824732780457},\n",
       "    {'tok_str': ' considered', 'prob': 0.2982138395309448},\n",
       "    {'tok_str': ' looked', 'prob': 0.03622495010495186},\n",
       "    {'tok_str': ' directly', 'prob': 0.022337431088089943},\n",
       "    {'tok_str': ' only', 'prob': 0.02055145427584648},\n",
       "    {'tok_str': ' simply', 'prob': 0.014589851722121239},\n",
       "    {'tok_str': ' converted', 'prob': 0.0},\n",
       "    {'tok_str': ' used', 'prob': 0.0},\n",
       "    {'tok_str': ' noticed', 'prob': 0.0},\n",
       "    {'tok_str': ' also', 'prob': 0.0}]},\n",
       "  {'content': ' their',\n",
       "   'probs': [{'tok_str': ' the', 'prob': 0.8232892751693726},\n",
       "    {'tok_str': ' them', 'prob': 0.14308013021945953},\n",
       "    {'tok_str': ' their', 'prob': 0.03363063186407089},\n",
       "    {'tok_str': ' ', 'prob': 0.0},\n",
       "    {'tok_str': ' whole', 'prob': 0.0},\n",
       "    {'tok_str': ' only', 'prob': 0.0},\n",
       "    {'tok_str': ' each', 'prob': 0.0},\n",
       "    {'tok_str': ' just', 'prob': 0.0},\n",
       "    {'tok_str': ' these', 'prob': 0.0},\n",
       "    {'tok_str': ' both', 'prob': 0.0}]},\n",
       "  {'content': ' integer',\n",
       "   'probs': [{'tok_str': ' whole', 'prob': 0.558951199054718},\n",
       "    {'tok_str': ' integer', 'prob': 0.3358916640281677},\n",
       "    {'tok_str': ' fractional', 'prob': 0.05863288417458534},\n",
       "    {'tok_str': ' decimal', 'prob': 0.032023798674345016},\n",
       "    {'tok_str': ' numerical', 'prob': 0.014500372111797333},\n",
       "    {'tok_str': ' rounded', 'prob': 0.0},\n",
       "    {'tok_str': ' absolute', 'prob': 0.0},\n",
       "    {'tok_str': ' exact', 'prob': 0.0},\n",
       "    {'tok_str': ' fractions', 'prob': 0.0},\n",
       "    {'tok_str': ' string', 'prob': 0.0}]},\n",
       "  {'content': ' parts',\n",
       "   'probs': [{'tok_str': ' parts', 'prob': 0.9711436629295349},\n",
       "    {'tok_str': ' values', 'prob': 0.028856365010142326},\n",
       "    {'tok_str': ' components', 'prob': 0.0},\n",
       "    {'tok_str': ' representations', 'prob': 0.0},\n",
       "    {'tok_str': ' equivalents', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': ' part', 'prob': 0.0},\n",
       "    {'tok_str': ' or', 'prob': 0.0},\n",
       "    {'tok_str': ' forms', 'prob': 0.0},\n",
       "    {'tok_str': ' expansions', 'prob': 0.0}]},\n",
       "  {'content': ' (',\n",
       "   'probs': [{'tok_str': ' (', 'prob': 0.3847009837627411},\n",
       "    {'tok_str': ' first', 'prob': 0.19460365176200867},\n",
       "    {'tok_str': ' and', 'prob': 0.14955654740333557},\n",
       "    {'tok_str': ',', 'prob': 0.09406667202711105},\n",
       "    {'tok_str': '.', 'prob': 0.05224994570016861},\n",
       "    {'tok_str': ' as', 'prob': 0.03250330314040184},\n",
       "    {'tok_str': ' before', 'prob': 0.02845991961658001},\n",
       "    {'tok_str': ' or', 'prob': 0.02822740375995636},\n",
       "    {'tok_str': ':', 'prob': 0.023956241086125374},\n",
       "    {'tok_str': ' separately', 'prob': 0.011675228364765644}]},\n",
       "  {'content': '9',\n",
       "   'probs': [{'tok_str': '9', 'prob': 0.9313841462135315},\n",
       "    {'tok_str': 'the', 'prob': 0.06861582398414612},\n",
       "    {'tok_str': 'whole', 'prob': 0.0},\n",
       "    {'tok_str': 'in', 'prob': 0.0},\n",
       "    {'tok_str': 'e', 'prob': 0.0},\n",
       "    {'tok_str': 'i', 'prob': 0.0},\n",
       "    {'tok_str': 'before', 'prob': 0.0},\n",
       "    {'tok_str': '8', 'prob': 0.0},\n",
       "    {'tok_str': 'if', 'prob': 0.0},\n",
       "    {'tok_str': '3', 'prob': 0.0}]},\n",
       "  {'content': ' and',\n",
       "   'probs': [{'tok_str': ' and', 'prob': 0.8500090837478638},\n",
       "    {'tok_str': ')', 'prob': 0.05012015253305435},\n",
       "    {'tok_str': ' for', 'prob': 0.0400618351995945},\n",
       "    {'tok_str': ' +', 'prob': 0.03187216445803642},\n",
       "    {'tok_str': 's', 'prob': 0.027936799451708794},\n",
       "    {'tok_str': '.', 'prob': 0.0},\n",
       "    {'tok_str': ' in', 'prob': 0.0},\n",
       "    {'tok_str': ' vs', 'prob': 0.0},\n",
       "    {'tok_str': '),', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0}]},\n",
       "  {'content': ' ',\n",
       "   'probs': [{'tok_str': ' ', 'prob': 1.0},\n",
       "    {'tok_str': ' the', 'prob': 0.0},\n",
       "    {'tok_str': ' both', 'prob': 0.0},\n",
       "    {'tok_str': ' -', 'prob': 0.0},\n",
       "    {'tok_str': ' a', 'prob': 0.0},\n",
       "    {'tok_str': ' not', 'prob': 0.0},\n",
       "    {'tok_str': ' either', 'prob': 0.0},\n",
       "    {'tok_str': ' no', 'prob': 0.0},\n",
       "    {'tok_str': ' only', 'prob': 0.0},\n",
       "    {'tok_str': ' also', 'prob': 0.0}]},\n",
       "  {'content': '9',\n",
       "   'probs': [{'tok_str': '9', 'prob': 1.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': '11', 'prob': 0.0},\n",
       "    {'tok_str': '0', 'prob': 0.0},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': '91', 'prob': 0.0},\n",
       "    {'tok_str': '99', 'prob': 0.0},\n",
       "    {'tok_str': '8', 'prob': 0.0},\n",
       "    {'tok_str': '90', 'prob': 0.0},\n",
       "    {'tok_str': '09', 'prob': 0.0}]},\n",
       "  {'content': ')',\n",
       "   'probs': [{'tok_str': ')', 'prob': 0.8525882363319397},\n",
       "    {'tok_str': '),', 'prob': 0.11883428692817688},\n",
       "    {'tok_str': ',', 'prob': 0.028577402234077454},\n",
       "    {'tok_str': ').', 'prob': 0.0},\n",
       "    {'tok_str': ' respectively', 'prob': 0.0},\n",
       "    {'tok_str': ' in', 'prob': 0.0},\n",
       "    {'tok_str': ' for', 'prob': 0.0},\n",
       "    {'tok_str': '.', 'prob': 0.0},\n",
       "    {'tok_str': ' +', 'prob': 0.0},\n",
       "    {'tok_str': '+', 'prob': 0.0}]},\n",
       "  {'content': ' and',\n",
       "   'probs': [{'tok_str': ' and', 'prob': 0.4926600754261017},\n",
       "    {'tok_str': ' before', 'prob': 0.3279683589935303},\n",
       "    {'tok_str': ' to', 'prob': 0.07571690529584885},\n",
       "    {'tok_str': ' first', 'prob': 0.06479647755622864},\n",
       "    {'tok_str': ' as', 'prob': 0.02085929736495018},\n",
       "    {'tok_str': ' separately', 'prob': 0.017998788505792618},\n",
       "    {'tok_str': ' or', 'prob': 0.0},\n",
       "    {'tok_str': ' since', 'prob': 0.0},\n",
       "    {'tok_str': ' instead', 'prob': 0.0},\n",
       "    {'tok_str': ' directly', 'prob': 0.0}]},\n",
       "  {'content': ' then',\n",
       "   'probs': [{'tok_str': ' then', 'prob': 0.8396791219711304},\n",
       "    {'tok_str': ' their', 'prob': 0.10840228945016861},\n",
       "    {'tok_str': ' the', 'prob': 0.05191861838102341},\n",
       "    {'tok_str': ' decimal', 'prob': 0.0},\n",
       "    {'tok_str': ' seen', 'prob': 0.0},\n",
       "    {'tok_str': ' considered', 'prob': 0.0},\n",
       "    {'tok_str': ' only', 'prob': 0.0},\n",
       "    {'tok_str': ' added', 'prob': 0.0},\n",
       "    {'tok_str': ' fractional', 'prob': 0.0},\n",
       "    {'tok_str': ' looked', 'prob': 0.0}]},\n",
       "  {'content': ' compared',\n",
       "   'probs': [{'tok_str': ' added', 'prob': 0.3941476047039032},\n",
       "    {'tok_str': ' considered', 'prob': 0.3080157935619354},\n",
       "    {'tok_str': ' compared', 'prob': 0.19442251324653625},\n",
       "    {'tok_str': ' looked', 'prob': 0.030520852655172348},\n",
       "    {'tok_str': ' the', 'prob': 0.019992675632238388},\n",
       "    {'tok_str': ' combined', 'prob': 0.016725962981581688},\n",
       "    {'tok_str': ' determined', 'prob': 0.015088529326021671},\n",
       "    {'tok_str': ' decided', 'prob': 0.011653152294456959},\n",
       "    {'tok_str': ' accounted', 'prob': 0.009433016180992126},\n",
       "    {'tok_str': ' rounded', 'prob': 0.0}]},\n",
       "  {'content': ' their',\n",
       "   'probs': [{'tok_str': ' the', 'prob': 0.8116979002952576},\n",
       "    {'tok_str': ' their', 'prob': 0.1019408330321312},\n",
       "    {'tok_str': ' those', 'prob': 0.08636125177145004},\n",
       "    {'tok_str': ' any', 'prob': 0.0},\n",
       "    {'tok_str': ' ', 'prob': 0.0},\n",
       "    {'tok_str': ' them', 'prob': 0.0},\n",
       "    {'tok_str': ' that', 'prob': 0.0},\n",
       "    {'tok_str': ' these', 'prob': 0.0},\n",
       "    {'tok_str': ' which', 'prob': 0.0},\n",
       "    {'tok_str': ' each', 'prob': 0.0}]},\n",
       "  {'content': ' fractional',\n",
       "   'probs': [{'tok_str': ' fractional', 'prob': 0.6478356122970581},\n",
       "    {'tok_str': ' decimal', 'prob': 0.3521643579006195},\n",
       "    {'tok_str': ' decimals', 'prob': 0.0},\n",
       "    {'tok_str': ' fractions', 'prob': 0.0},\n",
       "    {'tok_str': ' non', 'prob': 0.0},\n",
       "    {'tok_str': ' differences', 'prob': 0.0},\n",
       "    {'tok_str': ' floating', 'prob': 0.0},\n",
       "    {'tok_str': ' integer', 'prob': 0.0},\n",
       "    {'tok_str': ' additional', 'prob': 0.0},\n",
       "    {'tok_str': ' digits', 'prob': 0.0}]},\n",
       "  {'content': ' parts',\n",
       "   'probs': [{'tok_str': ' parts', 'prob': 1.0},\n",
       "    {'tok_str': ' part', 'prob': 0.0},\n",
       "    {'tok_str': ' components', 'prob': 0.0},\n",
       "    {'tok_str': ' expansions', 'prob': 0.0},\n",
       "    {'tok_str': ' or', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0},\n",
       "    {'tok_str': ' increments', 'prob': 0.0},\n",
       "    {'tok_str': ' decimals', 'prob': 0.0},\n",
       "    {'tok_str': ' decimal', 'prob': 0.0},\n",
       "    {'tok_str': 'parts', 'prob': 0.0}]},\n",
       "  {'content': ' (',\n",
       "   'probs': [{'tok_str': ' (', 'prob': 0.7105147838592529},\n",
       "    {'tok_str': '.', 'prob': 0.257625550031662},\n",
       "    {'tok_str': ' (.', 'prob': 0.031859613955020905},\n",
       "    {'tok_str': ' separately', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': ' to', 'prob': 0.0},\n",
       "    {'tok_str': ' if', 'prob': 0.0},\n",
       "    {'tok_str': ' as', 'prob': 0.0},\n",
       "    {'tok_str': '.\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': ' (<', 'prob': 0.0}]},\n",
       "  {'content': '0',\n",
       "   'probs': [{'tok_str': '0', 'prob': 1.0},\n",
       "    {'tok_str': 'the', 'prob': 0.0},\n",
       "    {'tok_str': 'which', 'prob': 0.0},\n",
       "    {'tok_str': '9', 'prob': 0.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': 'both', 'prob': 0.0},\n",
       "    {'tok_str': 'or', 'prob': 0.0},\n",
       "    {'tok_str': 'including', 'prob': 0.0},\n",
       "    {'tok_str': 'a', 'prob': 0.0},\n",
       "    {'tok_str': ' decimals', 'prob': 0.0}]},\n",
       "  {'content': '.',\n",
       "   'probs': [{'tok_str': '.', 'prob': 1.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': ' for', 'prob': 0.0},\n",
       "    {'tok_str': ' vs', 'prob': 0.0},\n",
       "    {'tok_str': ' +', 'prob': 0.0},\n",
       "    {'tok_str': ' in', 'prob': 0.0},\n",
       "    {'tok_str': 's', 'prob': 0.0},\n",
       "    {'tok_str': '.x', 'prob': 0.0},\n",
       "    {'tok_str': ')', 'prob': 0.0}]},\n",
       "  {'content': '1',\n",
       "   'probs': [{'tok_str': '1', 'prob': 1.0},\n",
       "    {'tok_str': '9', 'prob': 0.0},\n",
       "    {'tok_str': '01', 'prob': 0.0},\n",
       "    {'tok_str': '09', 'prob': 0.0},\n",
       "    {'tok_str': '0', 'prob': 0.0},\n",
       "    {'tok_str': '099', 'prob': 0.0},\n",
       "    {'tok_str': '99', 'prob': 0.0},\n",
       "    {'tok_str': '11', 'prob': 0.0},\n",
       "    {'tok_str': '001', 'prob': 0.0},\n",
       "    {'tok_str': '000', 'prob': 0.0}]},\n",
       "  {'content': ' and',\n",
       "   'probs': [{'tok_str': ' and', 'prob': 1.0},\n",
       "    {'tok_str': ' vs', 'prob': 0.0},\n",
       "    {'tok_str': ' for', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': ').', 'prob': 0.0},\n",
       "    {'tok_str': ' in', 'prob': 0.0},\n",
       "    {'tok_str': ' each', 'prob': 0.0},\n",
       "    {'tok_str': ')', 'prob': 0.0},\n",
       "    {'tok_str': ' +', 'prob': 0.0},\n",
       "    {'tok_str': ' -', 'prob': 0.0}]},\n",
       "  {'content': ' ',\n",
       "   'probs': [{'tok_str': ' ', 'prob': 1.0},\n",
       "    {'tok_str': ' another', 'prob': 0.0},\n",
       "    {'tok_str': ' .', 'prob': 0.0},\n",
       "    {'tok_str': ' the', 'prob': 0.0},\n",
       "    {'tok_str': ' an', 'prob': 0.0},\n",
       "    {'tok_str': ' no', 'prob': 0.0},\n",
       "    {'tok_str': ' also', 'prob': 0.0},\n",
       "    {'tok_str': ' +', 'prob': 0.0},\n",
       "    {'tok_str': ' additional', 'prob': 0.0},\n",
       "    {'tok_str': ' a', 'prob': 0.0}]},\n",
       "  {'content': '0',\n",
       "   'probs': [{'tok_str': '0', 'prob': 1.0},\n",
       "    {'tok_str': '1', 'prob': 0.0},\n",
       "    {'tok_str': '2', 'prob': 0.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': '3', 'prob': 0.0},\n",
       "    {'tok_str': '4', 'prob': 0.0},\n",
       "    {'tok_str': '5', 'prob': 0.0},\n",
       "    {'tok_str': '9', 'prob': 0.0},\n",
       "    {'tok_str': '6', 'prob': 0.0},\n",
       "    {'tok_str': ' ', 'prob': 0.0}]},\n",
       "  {'content': '.',\n",
       "   'probs': [{'tok_str': '.', 'prob': 1.0},\n",
       "    {'tok_str': ').', 'prob': 0.0},\n",
       "    {'tok_str': ')', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': '),', 'prob': 0.0},\n",
       "    {'tok_str': ' +', 'prob': 0.0},\n",
       "    {'tok_str': '+', 'prob': 0.0},\n",
       "    {'tok_str': ').\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': '.)', 'prob': 0.0},\n",
       "    {'tok_str': '.).', 'prob': 0.0}]},\n",
       "  {'content': '1',\n",
       "   'probs': [{'tok_str': '1', 'prob': 1.0},\n",
       "    {'tok_str': '11', 'prob': 0.0},\n",
       "    {'tok_str': '01', 'prob': 0.0},\n",
       "    {'tok_str': '0', 'prob': 0.0},\n",
       "    {'tok_str': '2', 'prob': 0.0},\n",
       "    {'tok_str': '001', 'prob': 0.0},\n",
       "    {'tok_str': '10', 'prob': 0.0},\n",
       "    {'tok_str': '100', 'prob': 0.0},\n",
       "    {'tok_str': '09', 'prob': 0.0},\n",
       "    {'tok_str': '099', 'prob': 0.0}]},\n",
       "  {'content': '),',\n",
       "   'probs': [{'tok_str': ').', 'prob': 0.8848121166229248},\n",
       "    {'tok_str': ')', 'prob': 0.06639394909143448},\n",
       "    {'tok_str': '),', 'prob': 0.04879390075802803},\n",
       "    {'tok_str': ').\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': ' respectively', 'prob': 0.0},\n",
       "    {'tok_str': ' +', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': '+', 'prob': 0.0},\n",
       "    {'tok_str': ' in', 'prob': 0.0}]},\n",
       "  {'content': ' but',\n",
       "   'probs': [{'tok_str': ' which', 'prob': 0.7335200309753418},\n",
       "    {'tok_str': ' as', 'prob': 0.0791306123137474},\n",
       "    {'tok_str': ' but', 'prob': 0.06451557576656342},\n",
       "    {'tok_str': ' since', 'prob': 0.04334329441189766},\n",
       "    {'tok_str': ' resulting', 'prob': 0.035755034536123276},\n",
       "    {'tok_str': ' or', 'prob': 0.023748671635985374},\n",
       "    {'tok_str': ' and', 'prob': 0.019986756145954132},\n",
       "    {'tok_str': ' leading', 'prob': 0.0},\n",
       "    {'tok_str': ' where', 'prob': 0.0},\n",
       "    {'tok_str': ' respectively', 'prob': 0.0}]},\n",
       "  {'content': ' that',\n",
       "   'probs': [{'tok_str': ' since', 'prob': 0.381422221660614},\n",
       "    {'tok_str': ' the', 'prob': 0.19298627972602844},\n",
       "    {'tok_str': ' that', 'prob': 0.14803971350193024},\n",
       "    {'tok_str': ' in', 'prob': 0.08342519402503967},\n",
       "    {'tok_str': ' I', 'prob': 0.07700440287590027},\n",
       "    {'tok_str': ' this', 'prob': 0.028092127293348312},\n",
       "    {'tok_str': ' it', 'prob': 0.022765543311834335},\n",
       "    {'tok_str': ' ultimately', 'prob': 0.02040736749768257},\n",
       "    {'tok_str': ' as', 'prob': 0.018791113048791885},\n",
       "    {'tok_str': ' my', 'prob': 0.013546818867325783}]},\n",
       "  {'content': ' didn',\n",
       "   'probs': [{'tok_str': \"'s\", 'prob': 0.3545699119567871},\n",
       "    {'tok_str': ' was', 'prob': 0.23284508287906647},\n",
       "    {'tok_str': ' would', 'prob': 0.2060612291097641},\n",
       "    {'tok_str': ' wasn', 'prob': 0.03577250614762306},\n",
       "    {'tok_str': ' didn', 'prob': 0.033061765134334564},\n",
       "    {'tok_str': ' still', 'prob': 0.03275452181696892},\n",
       "    {'tok_str': ' also', 'prob': 0.03127414360642433},\n",
       "    {'tok_str': ' is', 'prob': 0.030791042372584343},\n",
       "    {'tok_str': ' doesn', 'prob': 0.028219416737556458},\n",
       "    {'tok_str': ' led', 'prob': 0.014650377444922924}]},\n",
       "  {'content': \"'t\",\n",
       "   'probs': [{'tok_str': \"'t\", 'prob': 1.0},\n",
       "    {'tok_str': '’t', 'prob': 0.0},\n",
       "    {'tok_str': \"''\", 'prob': 0.0},\n",
       "    {'tok_str': \"'\", 'prob': 0.0},\n",
       "    {'tok_str': \"'T\", 'prob': 0.0},\n",
       "    {'tok_str': ' the', 'prob': 0.0},\n",
       "    {'tok_str': \"\\\\'\", 'prob': 0.0},\n",
       "    {'tok_str': ' not', 'prob': 0.0},\n",
       "    {'tok_str': ' this', 'prob': 0.0},\n",
       "    {'tok_str': ';t', 'prob': 0.0}]},\n",
       "  {'content': ' change',\n",
       "   'probs': [{'tok_str': ' change', 'prob': 0.8151849508285522},\n",
       "    {'tok_str': ' affect', 'prob': 0.07865209132432938},\n",
       "    {'tok_str': ' make', 'prob': 0.057893842458724976},\n",
       "    {'tok_str': ' lead', 'prob': 0.04826907813549042},\n",
       "    {'tok_str': ' matter', 'prob': 0.0},\n",
       "    {'tok_str': ' help', 'prob': 0.0},\n",
       "    {'tok_str': ' seem', 'prob': 0.0},\n",
       "    {'tok_str': ' yield', 'prob': 0.0},\n",
       "    {'tok_str': ' provide', 'prob': 0.0},\n",
       "    {'tok_str': ' alter', 'prob': 0.0}]},\n",
       "  {'content': ' the',\n",
       "   'probs': [{'tok_str': ' the', 'prob': 1.0},\n",
       "    {'tok_str': ' my', 'prob': 0.0},\n",
       "    {'tok_str': ' their', 'prob': 0.0},\n",
       "    {'tok_str': ' our', 'prob': 0.0},\n",
       "    {'tok_str': ' either', 'prob': 0.0},\n",
       "    {'tok_str': ' anything', 'prob': 0.0},\n",
       "    {'tok_str': ' much', 'prob': 0.0},\n",
       "    {'tok_str': ' what', 'prob': 0.0},\n",
       "    {'tok_str': ' who', 'prob': 0.0},\n",
       "    {'tok_str': ' any', 'prob': 0.0}]},\n",
       "  {'content': ' outcome',\n",
       "   'probs': [{'tok_str': ' outcome', 'prob': 0.3532424867153168},\n",
       "    {'tok_str': ' result', 'prob': 0.3357881009578705},\n",
       "    {'tok_str': ' fact', 'prob': 0.10652100294828415},\n",
       "    {'tok_str': ' final', 'prob': 0.06760498881340027},\n",
       "    {'tok_str': ' initial', 'prob': 0.041975852102041245},\n",
       "    {'tok_str': ' answer', 'prob': 0.03739467263221741},\n",
       "    {'tok_str': ' conclusion', 'prob': 0.03227793797850609},\n",
       "    {'tok_str': ' original', 'prob': 0.025194965302944183},\n",
       "    {'tok_str': ' overall', 'prob': 0.0},\n",
       "    {'tok_str': ' end', 'prob': 0.0}]},\n",
       "  {'content': '.',\n",
       "   'probs': [{'tok_str': '.', 'prob': 0.748002290725708},\n",
       "    {'tok_str': ' either', 'prob': 0.2519977390766144},\n",
       "    {'tok_str': '.\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': ' anyway', 'prob': 0.0},\n",
       "    {'tok_str': ' as', 'prob': 0.0},\n",
       "    {'tok_str': ' since', 'prob': 0.0},\n",
       "    {'tok_str': '...', 'prob': 0.0},\n",
       "    {'tok_str': '.</', 'prob': 0.0},\n",
       "    {'tok_str': ' (', 'prob': 0.0}]},\n",
       "  {'content': ' It',\n",
       "   'probs': [{'tok_str': '<|eot_id|>', 'prob': 0.20626303553581238},\n",
       "    {'tok_str': ' I', 'prob': 0.19219925999641418},\n",
       "    {'tok_str': ' The', 'prob': 0.17375119030475616},\n",
       "    {'tok_str': ' This', 'prob': 0.1281767189502716},\n",
       "    {'tok_str': ' A', 'prob': 0.06556135416030884},\n",
       "    {'tok_str': ' It', 'prob': 0.05113261938095093},\n",
       "    {'tok_str': ' To', 'prob': 0.0478501170873642},\n",
       "    {'tok_str': ' My', 'prob': 0.032025329768657684},\n",
       "    {'tok_str': ' However', 'prob': 0.031797993928194046},\n",
       "    {'tok_str': ' <', 'prob': 0.021632833406329155}]},\n",
       "  {'content': ' was',\n",
       "   'probs': [{'tok_str': \"'s\", 'prob': 0.48308974504470825},\n",
       "    {'tok_str': ' seems', 'prob': 0.2167159914970398},\n",
       "    {'tok_str': ' was', 'prob': 0.1655469387769699},\n",
       "    {'tok_str': ' turns', 'prob': 0.06345708668231964},\n",
       "    {'tok_str': ' just', 'prob': 0.03249216452240944},\n",
       "    {'tok_str': ' took', 'prob': 0.02227400243282318},\n",
       "    {'tok_str': ' would', 'prob': 0.016424039378762245},\n",
       "    {'tok_str': ' looks', 'prob': 0.0},\n",
       "    {'tok_str': ' appears', 'prob': 0.0},\n",
       "    {'tok_str': ' takes', 'prob': 0.0}]},\n",
       "  {'content': ' a',\n",
       "   'probs': [{'tok_str': ' a', 'prob': 0.9084224104881287},\n",
       "    {'tok_str': ' an', 'prob': 0.06221684813499451},\n",
       "    {'tok_str': ' actually', 'prob': 0.029360808432102203},\n",
       "    {'tok_str': ' more', 'prob': 0.0},\n",
       "    {'tok_str': ' unnecessary', 'prob': 0.0},\n",
       "    {'tok_str': ' my', 'prob': 0.0},\n",
       "    {'tok_str': ' still', 'prob': 0.0},\n",
       "    {'tok_str': ' just', 'prob': 0.0},\n",
       "    {'tok_str': ' the', 'prob': 0.0},\n",
       "    {'tok_str': ' only', 'prob': 0.0}]},\n",
       "  {'content': ' mistake',\n",
       "   'probs': [{'tok_str': ' mistake', 'prob': 0.3612085282802582},\n",
       "    {'tok_str': ' good', 'prob': 0.16009710729122162},\n",
       "    {'tok_str': ' flawed', 'prob': 0.15522056818008423},\n",
       "    {'tok_str': ' clever', 'prob': 0.09393294900655746},\n",
       "    {'tok_str': ' simple', 'prob': 0.05041860416531563},\n",
       "    {'tok_str': ' minor', 'prob': 0.04281461238861084},\n",
       "    {'tok_str': ' logical', 'prob': 0.04020963981747627},\n",
       "    {'tok_str': ' more', 'prob': 0.02010408043861389},\n",
       "    {'tok_str': ' correct', 'prob': 0.01828550174832344},\n",
       "    {'tok_str': ' case', 'prob': 0.012575366534292698}]},\n",
       "  {'content': ' to',\n",
       "   'probs': [{'tok_str': ' to', 'prob': 0.7857642769813538},\n",
       "    {'tok_str': ' on', 'prob': 0.18589888513088226},\n",
       "    {'tok_str': ' in', 'prob': 0.02833688259124756},\n",
       "    {'tok_str': ' of', 'prob': 0.0},\n",
       "    {'tok_str': ' for', 'prob': 0.0},\n",
       "    {'tok_str': '.', 'prob': 0.0},\n",
       "    {'tok_str': ' not', 'prob': 0.0},\n",
       "    {'tok_str': ',', 'prob': 0.0},\n",
       "    {'tok_str': '!', 'prob': 0.0},\n",
       "    {'tok_str': ' when', 'prob': 0.0}]},\n",
       "  {'content': ' initially',\n",
       "   'probs': [{'tok_str': ' consider', 'prob': 0.24638400971889496},\n",
       "    {'tok_str': ' assume', 'prob': 0.1923329383134842},\n",
       "    {'tok_str': ' think', 'prob': 0.12387973815202713},\n",
       "    {'tok_str': ' conclude', 'prob': 0.09522750228643417},\n",
       "    {'tok_str': ' initially', 'prob': 0.07806174457073212},\n",
       "    {'tok_str': ' focus', 'prob': 0.07398057729005814},\n",
       "    {'tok_str': ' compare', 'prob': 0.042470186948776245},\n",
       "    {'tok_str': ' not', 'prob': 0.021342361345887184},\n",
       "    {'tok_str': ' rely', 'prob': 0.021130116656422615},\n",
       "    {'tok_str': ' use', 'prob': 0.01977161504328251}]},\n",
       "  {'content': ' consider',\n",
       "   'probs': [{'tok_str': ' conclude', 'prob': 0.3092866539955139},\n",
       "    {'tok_str': ' consider', 'prob': 0.3051154315471649},\n",
       "    {'tok_str': ' compare', 'prob': 0.12134106457233429},\n",
       "    {'tok_str': ' assume', 'prob': 0.09653361886739731},\n",
       "    {'tok_str': ' focus', 'prob': 0.06934817880392075},\n",
       "    {'tok_str': ' think', 'prob': 0.06186370179057121},\n",
       "    {'tok_str': ' treat', 'prob': 0.021702410653233528},\n",
       "    {'tok_str': ' use', 'prob': 0.014808908104896545},\n",
       "    {'tok_str': ' rely', 'prob': 0.0},\n",
       "    {'tok_str': ' ignore', 'prob': 0.0}]},\n",
       "  {'content': ' comparing',\n",
       "   'probs': [{'tok_str': ' comparing', 'prob': 0.5326377749443054},\n",
       "    {'tok_str': ' decimal', 'prob': 0.21864663064479828},\n",
       "    {'tok_str': ' the', 'prob': 0.16396254301071167},\n",
       "    {'tok_str': ' decimals', 'prob': 0.06244528293609619},\n",
       "    {'tok_str': ' only', 'prob': 0.022307803854346275},\n",
       "    {'tok_str': ' that', 'prob': 0.0},\n",
       "    {'tok_str': ' using', 'prob': 0.0},\n",
       "    {'tok_str': ' ', 'prob': 0.0},\n",
       "    {'tok_str': ' adding', 'prob': 0.0},\n",
       "    {'tok_str': ' just', 'prob': 0.0}]},\n",
       "  {'content': ' the',\n",
       "   'probs': [{'tok_str': ' the', 'prob': 0.5927883386611938},\n",
       "    {'tok_str': ' decimals', 'prob': 0.32814648747444153},\n",
       "    {'tok_str': ' just', 'prob': 0.032424721866846085},\n",
       "    {'tok_str': ' decimal', 'prob': 0.030636928975582123},\n",
       "    {'tok_str': ' only', 'prob': 0.016003558412194252},\n",
       "    {'tok_str': ' them', 'prob': 0.0},\n",
       "    {'tok_str': ' as', 'prob': 0.0},\n",
       "    {'tok_str': ' their', 'prob': 0.0},\n",
       "    {'tok_str': ' numbers', 'prob': 0.0},\n",
       "    {'tok_str': ' whole', 'prob': 0.0}]},\n",
       "  {'content': ' numbers',\n",
       "   'probs': [{'tok_str': ' decimals', 'prob': 0.3973013460636139},\n",
       "    {'tok_str': ' numbers', 'prob': 0.37862488627433777},\n",
       "    {'tok_str': ' decimal', 'prob': 0.22407379746437073},\n",
       "    {'tok_str': ' whole', 'prob': 0.0},\n",
       "    {'tok_str': ' entire', 'prob': 0.0},\n",
       "    {'tok_str': ' digits', 'prob': 0.0},\n",
       "    {'tok_str': ' integers', 'prob': 0.0},\n",
       "    {'tok_str': ' fractions', 'prob': 0.0},\n",
       "    {'tok_str': ' full', 'prob': 0.0},\n",
       "    {'tok_str': ' exact', 'prob': 0.0}]},\n",
       "  {'content': ' as',\n",
       "   'probs': [{'tok_str': ' as', 'prob': 1.0},\n",
       "    {'tok_str': ' directly', 'prob': 0.0},\n",
       "    {'tok_str': ' in', 'prob': 0.0},\n",
       "    {'tok_str': ' solely', 'prob': 0.0},\n",
       "    {'tok_str': '.', 'prob': 0.0},\n",
       "    {'tok_str': ' themselves', 'prob': 0.0},\n",
       "    {'tok_str': ' without', 'prob': 0.0},\n",
       "    {'tok_str': ' just', 'prob': 0.0},\n",
       "    {'tok_str': ' to', 'prob': 0.0},\n",
       "    {'tok_str': ' exactly', 'prob': 0.0}]},\n",
       "  {'content': ' decimals',\n",
       "   'probs': [{'tok_str': ' decimals', 'prob': 1.0},\n",
       "    {'tok_str': ' whole', 'prob': 0.0},\n",
       "    {'tok_str': ' decimal', 'prob': 0.0},\n",
       "    {'tok_str': ' if', 'prob': 0.0},\n",
       "    {'tok_str': ' just', 'prob': 0.0},\n",
       "    {'tok_str': ' integers', 'prob': 0.0},\n",
       "    {'tok_str': ' they', 'prob': 0.0},\n",
       "    {'tok_str': ' a', 'prob': 0.0},\n",
       "    {'tok_str': ' strings', 'prob': 0.0},\n",
       "    {'tok_str': ' digits', 'prob': 0.0}]},\n",
       "  {'content': '.',\n",
       "   'probs': [{'tok_str': '.', 'prob': 0.9731253385543823},\n",
       "    {'tok_str': ',', 'prob': 0.026874614879488945},\n",
       "    {'tok_str': ' without', 'prob': 0.0},\n",
       "    {'tok_str': '.\\n\\n', 'prob': 0.0},\n",
       "    {'tok_str': '!', 'prob': 0.0},\n",
       "    {'tok_str': ' instead', 'prob': 0.0},\n",
       "    {'tok_str': ' when', 'prob': 0.0},\n",
       "    {'tok_str': ' and', 'prob': 0.0},\n",
       "    {'tok_str': '...', 'prob': 0.0},\n",
       "    {'tok_str': ' alone', 'prob': 0.0}]},\n",
       "  {'content': '<|eot_id|>',\n",
       "   'probs': [{'tok_str': '<|eot_id|>', 'prob': 0.5257210731506348},\n",
       "    {'tok_str': ' The', 'prob': 0.20127545297145844},\n",
       "    {'tok_str': ' <', 'prob': 0.12043420970439911},\n",
       "    {'tok_str': ' Thank', 'prob': 0.039531413465738297},\n",
       "    {'tok_str': ' \\n\\n', 'prob': 0.03304252400994301},\n",
       "    {'tok_str': ' I', 'prob': 0.03214592486619949},\n",
       "    {'tok_str': ' Instead', 'prob': 0.01738951914012432},\n",
       "    {'tok_str': ' To', 'prob': 0.016024865210056305},\n",
       "    {'tok_str': ' </', 'prob': 0.014435016550123692},\n",
       "    {'tok_str': ' A', 'prob': 0.0}]}]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<reflection> Ah, an easy one! \\n\\nLet's analyze the situation step by step:\\n\\n1. We have two decimal numbers: 9.9 and 9.11.\\n2. Since both numbers have the same decimal part (0.9), we can compare them by looking at the whole number part.\\n3. The whole number part of 9.9 is 9, and the whole number part of 9.11 is also 9.\\n4. Since both whole number\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1.3666777774741319, -1.3295235793825229)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"The quick brown fox jumps over the lazy dog.\"\n",
    "token_probs: List[TokenProb] = get_logprobs_completion(prompt, config)\n",
    "token_probs_list = [token_prob.token for token_prob in token_probs]\n",
    "# print(token_probs)\n",
    "XTC_sampler_probs = XTC_sampler(token_probs)\n",
    "print()\n",
    "# print(XTC_sampler_probs)\n",
    "XTC_sampler_probs_list = [token_prob.token for token_prob in XTC_sampler_probs]\n",
    "varentropy(token_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
